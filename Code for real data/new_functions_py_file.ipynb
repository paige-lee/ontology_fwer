{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ae7075",
   "metadata": {},
   "source": [
    "**The only function definitions that are modified are `phi()` and `estimate_P()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b65dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_adjacency_mat(adjacency_matrix):\n",
    "    ''' Function that cleans the adjacency matrix \n",
    "    \n",
    "    This function cleans the raw adjacency matrix so that parents come before children. \n",
    "    This converts between one convention and another convention. It doesn't check for parents vs. children.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix: pandas.DataFrame\n",
    "        A square adjacency matrix, where 1 in the (i, j) entry means the ith structure is a parent of the \n",
    "        jth structure.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pandas.DataFrame\n",
    "        A cleaned adjacency matrix (parents come before children)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Reverse the order of columns \n",
    "    columns = adjacency_matrix.columns.tolist()\n",
    "    columns = columns[::-1]\n",
    "    adjacency_matrix = adjacency_matrix[columns]\n",
    "    \n",
    "    # Reverse the order of rows\n",
    "    adjacency_matrix = adjacency_matrix[::-1]\n",
    "    \n",
    "    # Take the transpose of the matrix\n",
    "    adjacency_matrix = adjacency_matrix.T\n",
    "    \n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44dd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_multilevel(multilevel, adjacency_matrix):\n",
    "    \"\"\" Function that cleans the multilevel lookup table\n",
    "    \n",
    "    This function cleans the raw multilevel lookup table so that parents come before children. \n",
    "    The multilevel lookup table uses datasets from https://mricloud.org\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    multilevel: pandas.DataFrame\n",
    "        multilevel lookup table, whose columns are \"Structure,\" \"Immediate.parent,\" and \n",
    "        \"Immediate.child.children,\" and each row is a structure in the ontology. \n",
    "    \n",
    "    adjacency_matrix: pandas.DataFrame\n",
    "        A cleaned adjacency matrix (parents come before children)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pandas.DataFrame\n",
    "        Cleaned multilevel lookup table (parents come before children)\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    # Reverse the order of rows\n",
    "    multilevel = multilevel[::-1]\n",
    "    \n",
    "    # Reassign the numbers\n",
    "    multilevel.Number = range(1, adjacency_matrix.shape[0] + 1) \n",
    "    \n",
    "    return multilevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266dbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_matrix_creator(subset_leaf_list, adjacency_matrix, multilevel):\n",
    "    ''' Function that creates a subset matrix\n",
    "    \n",
    "    This function creates a subset of the adjacency matrix using the user-specified structures.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    subset_leaf_list: list\n",
    "        List of leaf structures to include in the subset\n",
    "    \n",
    "    adjacency_matrix: pandas.DataFrame\n",
    "        The cleaned adjacency matrix\n",
    "    \n",
    "    multilevel: pandas.DataFrame\n",
    "        The cleaned multilevel lookup table\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pandas.DataFrame\n",
    "        A square subset matrix of the adjacency matrix that includes the user-specified leaf structures and \n",
    "        all of their parents\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    full_subset_list = subset_leaf_list # This list will get filled\n",
    "    iterations = 5 * len(subset_leaf_list) - 1\n",
    "    \n",
    "    for i in range(0, len(subset_leaf_list)): # For each leaf structure\n",
    "        for j in range(0, iterations): # Iterate over the 4 other levels in this ontology (5 levels * number of leaves)\n",
    "\n",
    "            # Structure index\n",
    "            structure_index = int(np.where(multilevel[\"Structure\"] == full_subset_list[j])[0])\n",
    "            structure_index = (adjacency_matrix.shape[0]) - structure_index\n",
    "            structure = multilevel[\"Structure\"][structure_index]\n",
    "\n",
    "            full_subset_list.append(multilevel[\"Immediate.parent\"][structure_index])\n",
    "        \n",
    "    full_subset_list = set(full_subset_list)\n",
    "    full_subset_list = pd.DataFrame(full_subset_list)\n",
    "        \n",
    "    pattern = r\"_[0-9]+_\"\n",
    "    structure_numbers = []\n",
    "    \n",
    "    for i in range(0, full_subset_list.shape[0]):\n",
    "        if full_subset_list[0][i] == \"Everything\":\n",
    "            structure_numbers.append(float(\"inf\")) # Assign the number \"inf\" to \"Everything\" for flexibility\n",
    "        else:\n",
    "            number = re.findall(pattern, full_subset_list[0][i])[0]\n",
    "            number = re.sub(\"[^0-9]\", \"\", number)\n",
    "            number = int(number)\n",
    "            structure_numbers.append(number)\n",
    "    \n",
    "    full_subset_list[\"structure_numbers\"] = structure_numbers\n",
    "    full_subset_list = full_subset_list.sort_values(by = [\"structure_numbers\"], axis = 0)\n",
    "    full_subset_list = full_subset_list[0].tolist()\n",
    "    \n",
    "    # Index the rows and columns of the adjacency matrix by these structures to create a subset\n",
    "    subset = adjacency_matrix[full_subset_list]\n",
    "    subset = subset.loc[full_subset_list]\n",
    "\n",
    "    # Reverse the order of rows and columns in the subset of the adjacency matrix\n",
    "    cols = subset.columns.tolist()\n",
    "    cols = cols[::-1]\n",
    "    subset = subset[cols]\n",
    "    subset = subset[::-1]\n",
    "    \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f059af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_descendants(adjacency_matrix, N, mu):\n",
    "    ''' Function that creates an adjacency matrix of descendants\n",
    "    \n",
    "    Sometimes we're interesting in querying whether one structure is a descendant of the other as opposed to \n",
    "    a direct child.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    adjacency_matrix: binary numpy.array\n",
    "        The cleaned adjacency matrix\n",
    "    \n",
    "    N: int\n",
    "        The number of samples\n",
    "    \n",
    "    mu: float\n",
    "        The difference in means (generally unknown)\n",
    "    \n",
    "    Note: this function may only be used for an ontology that has 6 levels (\"Everything\" is the \n",
    "    highest/most general level).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    binary numpy.array\n",
    "        Transitive adjacency matrix \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    M = adjacency_matrix.shape[0] # Total number of unique structures\n",
    "    names_full = adjacency_matrix.columns # List of the 509 structures' names\n",
    "    A = np.array(adjacency_matrix, dtype = bool)\n",
    "    Descendants = np.copy(A)\n",
    "    \n",
    "    Descendants = np.logical_or(Descendants,Descendants@A)\n",
    "    Descendants = np.logical_or(Descendants,Descendants@A)\n",
    "    Descendants = np.logical_or(Descendants,Descendants@A)\n",
    "    Descendants = np.logical_or(Descendants,Descendants@A)\n",
    "    Descendants = np.logical_or(Descendants,Descendants@A)\n",
    "    Descendants = np.logical_or(Descendants,Descendants@A)\n",
    "    \n",
    "    return Descendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09116500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_ancestors(adjacency_matrix, N, mu):\n",
    "    ''' Function that creates an adjacency matrix of ancestors\n",
    "    \n",
    "    Sometimes we're interesting in querying whether one structure is an ancestor of the other as opposed to \n",
    "    a direct parent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix: pandas.DataFrame\n",
    "        The cleaned adjacency matrix\n",
    "    \n",
    "    N: int\n",
    "        The number of samples\n",
    "    \n",
    "    mu: float\n",
    "        The difference in means (generally unknown)\n",
    "    \n",
    "    Note: this function may only be used for an ontology that has 6 levels (\"Everything\" is the \n",
    "    highest/most general level).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    binary numpy.array\n",
    "        Transitive adjacency matrix \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    M = adjacency_matrix.shape[0]\n",
    "    Descendants = adjacency_descendants(adjacency_matrix, N, mu)\n",
    "    Ancestors = Descendants.T # Take transpose of descendants matrix to get ancestors    \n",
    "    Ancestors_and_self = np.logical_or(Ancestors,np.eye(M))\n",
    "    \n",
    "    return Ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x, mu = 0.0, sigma2 = 1.0):\n",
    "    ''' Standard normal distribution CDF\n",
    "    \n",
    "    A Gaussian probability density function with unit variance\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: float or numpy.array\n",
    "        A point to evaluate the function at, can be a numpy array\n",
    "    \n",
    "    mu: float\n",
    "        Mean of the gaussian (default 0); mu0 or mu1 may be inputted\n",
    "        \n",
    "    sigma2: float\n",
    "        Variance of the gaussian (default 1.0)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    numpy.array\n",
    "        The Gaussian probability density function evaluated at specified point\n",
    "    '''\n",
    "    return 1.0 / np.sqrt(2.0 * np.pi * sigma2) * np.exp(-(x - mu)**2 / (2.0 * sigma2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_from_Q(Q,Ancestors_and_self):\n",
    "    ''' Function that calculates P from Q \n",
    "    \n",
    "    This function computes the marginal probability that a structure is affected given the conditional \n",
    "    probabilities that structures are affected conidtioned on their parents.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q: numpy.array\n",
    "        List of conditional probabilities \n",
    "        \n",
    "    Ancestors_and_self: transitive adjacency matrix \n",
    "        Adjacency matrix with loops (when a node is connected to itself)\n",
    "    \n",
    "    Returns \n",
    "    ----------\n",
    "    numpy.array\n",
    "        marginal probabilities \n",
    "    \n",
    "    Note: this function is not called\n",
    "    \n",
    "    '''\n",
    "    P = np.empty_like(Q)\n",
    "    for i in range(M):\n",
    "        P[i] = np.prod(Q[Ancestors_and_self[i,:]])\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_from_P(P,A):\n",
    "    ''' Function that calculates Q from P\n",
    "    \n",
    "    Given a list of marginal probabilities of structures being affected, compute the conditional probabilities \n",
    "    of a structure being affected given its parents.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    P: numpy.array\n",
    "        The marignal probabilities\n",
    "    \n",
    "    A: binary numpy.array\n",
    "        Adjacency matrix that describes parent to child relationships\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    numpy.array\n",
    "        The conditional probabilities\n",
    "    \n",
    "    '''\n",
    "    M = A.shape[0]\n",
    "    # now we need to calculate Q\n",
    "    Q = np.zeros_like(P)\n",
    "    Q[0] = P[0]\n",
    "    for i in range(1,M):\n",
    "        Q[i] = P[i] / P[A[:,i]]\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1457d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mu_sigma_from_xp(X, P_ij, M):\n",
    "    '''Function for calculating mu and sigma^2 from X and P_ij\n",
    "    \n",
    "    This function calculates the MLE estimates of mu0, mu1, and sigma^2 (common variance)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy.array\n",
    "        The observed data for each leaf structure and each subject\n",
    "    \n",
    "    P_ij: numpy.array\n",
    "        The marginal probabilities that structures are affected for leaf structures only\n",
    "    \n",
    "    M: numpy.array\n",
    "        The total number of structures in the subset of the adjacency matrix \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    numpy.array\n",
    "        Returns three arrays, corresponding to the MLE estimates of mu0, mu1, and sigma^2 respectively\n",
    "    \n",
    "    '''\n",
    "    mu0i = sum(X * (1 - P_ij)) / sum(1 - P_ij)\n",
    "    mu1i = sum(P_ij * X) / sum(P_ij)\n",
    "    vi = (sum(P_ij * (X - mu1i)**2) + sum((1 - P_ij) * (X - mu0i)**2)) / M\n",
    "    return mu0i, mu1i, vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb2f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_P(X, A, Descendants_and_self, draw=False, niter=100, P0=None, names=None, clip=0.001, mu0 = 0.0, mu1 = 3.0, sigma2 = 1.0):\n",
    "    ''' Function for estimating P\n",
    "    \n",
    "    Apply an EM algorithm to estimate marginal probabiltiies that each structure is affected given a dataset.\n",
    "    We assume data is normally distributed with unit variance and mean mu.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy.array\n",
    "        Contains observations for each structure and each subject\n",
    "    \n",
    "    mu: float\n",
    "        The known mean for affected structures (unaffected structures have mean 0)\n",
    "    \n",
    "    A: binary numpy.array\n",
    "        Adjacency matrix describing parent child relationships\n",
    "    \n",
    "    Descendants_and_self: transitive adjacency matrix\n",
    "        Adjacency matrix of descendants. Can be computed from A, but here we use it as an input\n",
    "    \n",
    "    draw: int\n",
    "        Illustrates the data every `draw` iterations of EM algorithm. \n",
    "        draw = 0 or false means do not draw. Default value is False.\n",
    "    \n",
    "    iter: int\n",
    "        The number of iterations of em algorithm\n",
    "    \n",
    "    p0: float\n",
    "        The initial guess for marginal probabilities\n",
    "\n",
    "    names: list \n",
    "        The names (strings) of structures in ontology\n",
    "        \n",
    "    clip: float\n",
    "        Number that clips probabilities away from 0 or 1\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    numpy.array\n",
    "        Contains the marginal probabilities that structures are affected)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if draw: \n",
    "        f,ax = plt.subplots(2,2)\n",
    "        if names is None:\n",
    "            names = np.arange(A.shape[0])\n",
    "\n",
    "    N = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "    M = A.shape[0]\n",
    "    is_leaf = np.sum(Descendants_and_self, 1) == 1\n",
    "    \n",
    "    # okay now comes my algorithm\n",
    "    # initialize\n",
    "    if P0 is None:\n",
    "        P = np.ones(M)*0.5\n",
    "    else:\n",
    "        P = np.asarray(P0)\n",
    "    \n",
    "    for it in range(niter):\n",
    "        # calculate leaf posterior (this is prob of no effect)\n",
    "        #leaf_posterior = ((1.0-P[is_leaf])*phi(X))\n",
    "        #leaf_posterior = leaf_posterior/(leaf_posterior + P[is_leaf]*phi(X,mu) )\n",
    "        P_ = np.maximum(P, clip) # Clip probability: if P is very small, then set it to 0.001\n",
    "        P_ = np.minimum(P_, 1-clip) # Clip probability: if P_ is very big, then set it to 0.999\n",
    "        P_over_one_minus_P = P_/(1.0-P_)\n",
    "        #leaf_log_posterior = -np.log(1.0 + P_over_one_minus_P[is_leaf]*phi(X,mu)/phi(X) )\n",
    "        #leaf_log_posterior = -np.log1p(P_over_one_minus_P[is_leaf]*phi(X,mu)/phi(X))\n",
    "        leaf_log_posterior = -np.log1p(P_over_one_minus_P[is_leaf]*phi(X, mu1, sigma2)/phi(X, mu0, sigma2))\n",
    "        \n",
    "\n",
    "        # calculate posterior for all structures\n",
    "        # now for each structure, I need a leaf likelihod, and an adjustment\n",
    "        #posterior = np.zeros((N,M))\n",
    "        log_posterior = np.zeros((N,M))\n",
    "        for i in range(M):\n",
    "            #posterior[:,i] = np.prod(leaf_posterior[:,Descendants_and_self[i,:][is_leaf]],1)\n",
    "            log_posterior[:,i] = np.sum(leaf_log_posterior[:,Descendants_and_self[i,:][is_leaf]],1)\n",
    "        \n",
    "        # calculate adjustment factor for correlations\n",
    "        Q = Q_from_P(P,A)\n",
    "        #adjustment_single = np.ones(M)\n",
    "        log_adjustment_single = np.zeros(M)\n",
    "        for i in range(M):\n",
    "            if is_leaf[i]:\n",
    "                continue\n",
    "            #adjustment_single[i] = (1.0 - P[i])/ ((1.0 - P[i]) + P[i]*np.prod(1.0 - Q[A[i,:]]))\n",
    "            #log_adjustment_single[i] = -np.log(1.0 + P_over_one_minus_P[i]*np.prod(1.0 - Q[A[i,:]]))\n",
    "            log_adjustment_single[i] = -np.log1p(P_over_one_minus_P[i]*np.prod(1.0 - Q[A[i,:]]))\n",
    "            \n",
    "        \n",
    "        # now my adjust ment requres products of all descendants\n",
    "        #adjustment = np.ones(M)\n",
    "        log_adjustment = np.ones(M)\n",
    "        for i in range(M):\n",
    "            #adjustment[i] = np.prod(adjustment_single[Descendants_and_self[i,:]])\n",
    "            log_adjustment[i] = np.sum(log_adjustment_single[Descendants_and_self[i,:]])\n",
    "            \n",
    "\n",
    "        # calculate the adjusted posterior\n",
    "        #posterior = posterior*adjustment\n",
    "        log_posterior = log_posterior + log_adjustment\n",
    "        \n",
    "        #P = np.sum(1.0 - posterior,0)/N        \n",
    "        #P = np.sum(1.0 - np.exp(log_posterior),0)/N\n",
    "        P = -np.sum(np.expm1(log_posterior),0)/N\n",
    "        posterior = np.expm1(log_posterior)\n",
    "        \n",
    "        # Calculate the MLE estimates of mu0, mu1, and sigma2, and update their values\n",
    "        mle_estimates = calc_mu_sigma_from_xp(X, posterior, M) # Calculate MLE estimates of mu0, mu1, sigma^2\n",
    "        mu0 = mle_estimates[0]\n",
    "        mu1 = mle_estimates[1]\n",
    "        sigma2 = mle_estimates[2]\n",
    "        \n",
    "        # draw        \n",
    "        if draw>0 and ( (not it%draw) or (it==niter-1)):     \n",
    "            \n",
    "            ax[0,0].cla()\n",
    "            ax[0,0].imshow(posterior, vmin = 0, vmax = 1)\n",
    "            ax[0,0].set_aspect('auto')\n",
    "            ax[0,0].set_title('P[Z=0|X] (prob not affected)')\n",
    "            ax[0,0].set_xticks(np.arange(M))\n",
    "            ax[0,0].set_xticklabels(names,rotation=15, fontsize = 5)\n",
    "            ax[0,0].set_ylabel('Sample')\n",
    "\n",
    "            ax[0,1].cla()\n",
    "            ax[0,1].bar(np.arange(M),P)\n",
    "            ax[0,1].set_xticks(np.arange(M))\n",
    "            ax[0,1].set_xticklabels(names,rotation=15, fontsize = 5)\n",
    "            ax[0,1].set_ylim((0, 1))\n",
    "\n",
    "            f.canvas.draw()\n",
    "    return P, mu0, mu1, sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c550d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulated_data(filename, subset, case, n_repeats, N, mu):\n",
    "    ''' Function that generates simulated data\n",
    "    \n",
    "    This function generates the simulated data to be used for permutation testing\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: string\n",
    "        The user-specified name for the file names to save the generated data; one file will be saved for \n",
    "        each repeat; make sure to provide a file name that's different (i.e. say \"data\" rather than \"results\") \n",
    "        from the file name you'll specify for permutation testing results so they're separate files and won't be \n",
    "        overwritten; example: \"data.npz\" \n",
    "    \n",
    "    subset: pandas.DataFrame\n",
    "        The subset adjacency matrix\n",
    "    \n",
    "    case: int\n",
    "        The case number (1 = nothing is affected, 2 = left hippocampus is affected, 3 = both left hippocampus \n",
    "        and left amygdala are affected, 4 = either left hippocampus or left amygdala is affected but not both)\n",
    "    \n",
    "    n_repeats: int\n",
    "        The number of repeats. We want to generate a random dataset with the same parameters but `n_repeats` \n",
    "        different realizations of the random variables. n_repeats should be 1 in practice for external users, \n",
    "        but in our case, since we simulated a lot of data, n_repeats is greater than 1.\n",
    "    \n",
    "    N: int\n",
    "        The number of samples\n",
    "    \n",
    "    mu: float\n",
    "        The difference in means (assume it's known)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    npz file (written to disk, not explicitly returned)\n",
    "        The 1st array is X (probability of being affected for each sample), the 2nd array is Z (which \n",
    "        structures are affected or unaffected for each sample), and the 3rd array is G (whether each sample is \n",
    "        actually affected)\n",
    "  \n",
    "    '''\n",
    "    \n",
    "    M = subset.shape[0] # Number of total unique structures\n",
    "    basename, extension = os.path.splitext(filename)\n",
    "    \n",
    "    for j in range(n_repeats):\n",
    "        outputs = [] # Empty list for each iteration\n",
    "        Z = np.zeros((N,M)) # Initialize Z, which will be a binary variable that tells us if a structure is affected\n",
    "        Naffected = N // 2 # Don't set Naffected to 0 or else there won't be any samples\n",
    "        number_of_leaves = np.count_nonzero(np.sum(subset, 1) == 0) # Number of leaf structures (zero children)\n",
    "        \n",
    "        if case == 1:\n",
    "            pass\n",
    "        elif case == 2:\n",
    "            for i in range(N):\n",
    "                if i < Naffected: # Assume that the first half of samples are affected and second half are not\n",
    "                    Z[i][6] = 1 # Left hippocampus is affected\n",
    "        elif case == 3:\n",
    "            for i in range(N):\n",
    "                if i < Naffected: # Assume that the first half of samples are affected and second half are not\n",
    "                    Z[i][6] = 1 # Left hippocampus is affected\n",
    "                    Z[i][7] = 1 # Left amygdala is affected\n",
    "        elif case == 4:\n",
    "            for i in range(N):\n",
    "                if i < Naffected: # Assume that the first half of samples are affected and second half are not\n",
    "                    if np.random.rand() < 0.5:\n",
    "                        Z[i][6] = 1 # Left hippocampus is affected\n",
    "                    else:\n",
    "                        Z[i][7] = 1 # Left amygdala is affected\n",
    "            \n",
    "        is_leaf = np.concatenate([np.ones(number_of_leaves), np.zeros(M - number_of_leaves)]) # 1 for leaf structures, 0 for non-leaf structures\n",
    "        is_leaf = np.array(is_leaf, dtype = bool) # Convert is_leaf to the boolean type\n",
    "        is_leaf = is_leaf[::-1] # Data specific\n",
    "        m = np.sum(is_leaf) # Number of leaf structures (m = 2)\n",
    "            \n",
    "        G = np.arange(N) < Naffected # All falses since all samples are unaffected\n",
    "        X = Z[:, is_leaf > 0] * mu + np.random.randn(N, m)\n",
    "        \n",
    "        filename_new_this_repeat = basename + f'_repeat_{j:06d}' + extension\n",
    "        np.savez(filename_new_this_repeat, X = X, Z = Z, G = G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_function(input_string, dictionary):\n",
    "    ''' Function that sorts results by posterior probability\n",
    "    \n",
    "    This function sorts a list of strings by posterior probability, where parents come before children when \n",
    "    there are ties.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_string: list\n",
    "        A list of strings to be sorted; each string contains structure name, posterior probability, p-value\n",
    "    \n",
    "    dictionary: dictionary\n",
    "        A dictionary whose keys are the structures in the subset and values are 1 through the number of \n",
    "        structures in the subset (must be in order from parents to children)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        A sorted list of strings (sorted by posterior probability, where parents come before children when \n",
    "        there are ties)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    my_list = input_string\n",
    "    my_list = sorted(my_list, key = lambda x : dictionary[x.split(\",\")[0]])  \n",
    "    my_list = sorted(my_list, key = lambda x : float(x.split(\",\")[1].split(\"=\")[-1]), reverse = True) \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f732c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_testing(filename_old, filename_new, subset, n_repeats, nperm, N, mu, mu0, mu1, sigma2, niter, clip, initial_prob):\n",
    "    ''' Function that conducts permutation testing\n",
    "    \n",
    "    This function conducts permutation testing using the generated data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename_old: string\n",
    "        The pattern of the filenames of the generated data; example: \"test1_data_repeat_*\"\n",
    "    \n",
    "    filename_new: string\n",
    "        The user-specified filename pattern for the permutation testing results (choose a different name from \n",
    "        filename_old, i.e. say \"results\" instead of \"data\" if you don't want generated data to get overwritten \n",
    "        by permutation testing results); example: \"results.npz\"\n",
    "    \n",
    "    subset: pandas.DataFrame\n",
    "        The subset adjacency matrix\n",
    "    \n",
    "    n_repeats: int\n",
    "        The number of repeats. We generated a random dataset with the same parameters but `n_repeats` \n",
    "        different realizations of the random variables. `n_repeats` must be the same value as `n_repeats`\n",
    "        when we generated data earlier. n_repeats should be 1 in practice for external users, but in our case, \n",
    "        since we simulated a lot of data, n_repeats is greater than 1.\n",
    "    \n",
    "    nperm: int\n",
    "        The number of permutations for permutation testing \n",
    "    \n",
    "    N: int\n",
    "        The number of samples. N must be the same value as N from generating data earlier.\n",
    "    \n",
    "    mu: float\n",
    "        The difference in means (generally unknown). mu should be the same value as mu from generating data\n",
    "        earlier in order to get meaningful results. But, mu doesn't have to be the same if you don't want to\n",
    "        make it the same.\n",
    "    \n",
    "    niter: int\n",
    "        The number of iterations of the EM algorithm\n",
    "    \n",
    "    clip: float\n",
    "        Number that clips probabilities away from 0 or 1\n",
    "    \n",
    "    initial_prob: float\n",
    "        The intial probability\n",
    "    \n",
    "    Returns \n",
    "    ----------\n",
    "    npz file (written to disk, not explicitly returned)\n",
    "        The 1st array contains p-values, 2nd array contains the names of the structures in the subset, 3rd \n",
    "        array contains the posterior probabilities, and 4th array contains the information from the prior 3 \n",
    "        arrays saved in 1 string per structure.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    M = subset.shape[0] # Number of total unique structures\n",
    "    S = np.array(subset, dtype = bool)\n",
    "    names_subset = subset.columns # List of the 8 structures' names\n",
    "    Descendants = adjacency_descendants(subset, N=N, mu=mu)\n",
    "    Descendants_and_self = np.logical_or(Descendants, np.eye(M))\n",
    "    \n",
    "    basename, extension = os.path.splitext(filename_new)\n",
    "    filename_old = glob(filename_old)\n",
    "    filename_old = sorted(filename_old)\n",
    "    \n",
    "    for j in range(n_repeats):\n",
    "        \n",
    "        # Load the generated data for each repeat\n",
    "        data = np.load(filename_old[j])\n",
    "        X = data[\"X\"]\n",
    "        Z = data[\"Z\"]\n",
    "        G = data[\"G\"]\n",
    "        \n",
    "        outputs = [] # Empty list for each iteration\n",
    "        \n",
    "        ### PARAMETER ESTIMATION ###\n",
    "    \n",
    "        P_subset = np.ones(M) * 0.5 # Array of 8 copies of 0.5\n",
    "        Q = Q_from_P(P_subset, S)\n",
    "\n",
    "        P0 = np.ones(M) * initial_prob\n",
    "        P_subset = estimate_P(X[G], S, Descendants_and_self, draw=0, niter=niter, P0=P0, names=names_subset, clip=clip)\n",
    "        # Set draw = 0 to prevent drawing the graphs\n",
    "        \n",
    "        ### GENERATING PERMUTED DATA ###\n",
    "    \n",
    "        Ps = []\n",
    "        for n in range(nperm):\n",
    "            Xp = X[np.random.permutation(N)[G]]\n",
    "            P_ = estimate_P(Xp, S, Descendants_and_self,draw=0,niter=niter,P0=P0)\n",
    "            Ps.append(P_)\n",
    "\n",
    "        Ps_sort = np.array([np.sort(Pi)[::-1] for Pi in Ps])\n",
    "        \n",
    "        ### PERMUTATION TESTING ###\n",
    "    \n",
    "        inds = np.argsort(P_subset)[::-1]\n",
    "        pval = np.zeros_like(P_subset)\n",
    "        alpha = 0.05\n",
    "        \n",
    "        pval_list = [] # Empty list to be filled\n",
    "        names_list = [] # Empty list to be filled\n",
    "        posterior_list = [] # Empty list to be filled\n",
    "        \n",
    "        for i in range(M):    \n",
    "            pval[inds[i]] = np.mean(Ps_sort[:,i] >= P_subset[inds[i]])\n",
    "            outputs.append(f\"{names_subset[inds[i]]}, P[Z=1|X]={P_subset[inds[i]]}, p={pval[inds[i]]}\")\n",
    "            # Every structure that gets rejected gets an entry\n",
    "            \n",
    "            pval_list.append(pval[inds[i]])\n",
    "            names_list.append(names_subset[inds[i]])\n",
    "            posterior_list.append(P_subset[inds[i]])\n",
    "        \n",
    "        ### SORT THE POSTERIOR VALUES ###\n",
    "        \n",
    "        # Use the subset adjacency matrix to create a dictionary\n",
    "        columns = np.array(subset.columns)\n",
    "        dictionary = dict(enumerate(columns.flatten(), 1))\n",
    "        dictionary = dict((value, key) for key, value in dictionary.items()) # Swap the keys and values\n",
    "        outputs = sorting_function(outputs, dictionary)\n",
    "        \n",
    "        ### SAVE DATA ### \n",
    "        \n",
    "        filename_new_this_repeat = basename + f'_repeat_{j:06d}' + extension\n",
    "        np.savez(filename_new_this_repeat, pval = pval_list, names = names_list, posterior = posterior_list, strings = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0518547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positive_rate(subset, file_names, n_repeats):\n",
    "    ''' False positive rate function\n",
    "    \n",
    "    This function calculates the false positive rate after permutation testing\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subset: pandas.DataFrame\n",
    "        The subset adjacency matrix\n",
    "    \n",
    "    file_names: string\n",
    "        A string containing the file name pattern of the permutation testing results to be used in the \n",
    "        calculation\n",
    "    \n",
    "    n_repeats: int\n",
    "        The number of repeats. We generated a random dataset with the same parameters but `n_repeats` \n",
    "        different realizations of the random variables. `n_repeats` must be the same value for each file in the \n",
    "        file_name list. n_repeats should be 1 in practice for external users, but in our case, since we \n",
    "        simulated a lot of data, n_repeats is greater than 1.\n",
    "    \n",
    "    Note: we can only calculate false positive rates for cases 1 and 2; make sure the input files in the \n",
    "    file_name list were all created for the same case (either case 1 or case 2).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    float\n",
    "        The false positive rate from permutation testing\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    file_names = glob(file_names)\n",
    "    file_names = sorted(file_names)\n",
    "    \n",
    "    # Use the subset adjacency matrix to create a dictionary\n",
    "    columns = np.array(subset.columns)\n",
    "    dictionary = dict(enumerate(columns.flatten(), 1))\n",
    "    dictionary = dict((value, key) for key, value in dictionary.items()) # Swap the keys and values\n",
    "    \n",
    "    # False positive rate calculation\n",
    "    count = 0\n",
    "    for i in range(0, len(file_names)):\n",
    "        reject_p = np.zeros(len(columns)) # For each repeat, assume no structures are affected (null hypothesis)\n",
    "        file = np.load(file_names[i])\n",
    "        file = file[\"strings\"] # Values corresponding to the key\n",
    "        file = sorting_function(file, dictionary)\n",
    "        \n",
    "        for j in range(0, len(columns)): # For each structure in each repeat...\n",
    "            p = file[j].find(\"p=\") # Index of \"p\" in the string\n",
    "            pval = float(file[j][p::][2::]) # Extract the p-value\n",
    "            if pval < 0.05: # If the p-value is < 0.05...\n",
    "                reject_p[j] = 1\n",
    "            else: # The test statistic is the probabilities, which were sorted, so stop after the first structure we fail to reject\n",
    "                break\n",
    "                \n",
    "        if any(reject_p > 0): # If there's at least one structure with p < 0.05...\n",
    "            count += 1 # Add 1 to the false positive count\n",
    "                \n",
    "    return (count / n_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negative_rate(subset, file_names, case, n_repeats):\n",
    "    ''' False negative rate function\n",
    "    \n",
    "    This function calculates the false negative rate after permutation testing\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subset: pandas.DataFrame\n",
    "        The subset adjacency matrix\n",
    "    \n",
    "    file_names: string\n",
    "        A string containing the file name pattern of the permutation testing results to be used in the \n",
    "        calculation\n",
    "    \n",
    "    case: int\n",
    "        The case that was used to generate the data; all files in the file_names list must correspond to the \n",
    "        same case.\n",
    "    \n",
    "    n_repeats: int\n",
    "        The number of repeats. We generated a random dataset with the same parameters but `n_repeats` \n",
    "        different realizations of the random variables. `n_repeats` must be the same value for each file in the \n",
    "        file_name list. n_repeats should be 1 in practice for external users, but in our case, since we \n",
    "        simulated a lot of data, n_repeats is greater than 1.\n",
    "        \n",
    "    Note: we can only calculate false negative rates for cases 2, 3, and 4; make sure the input files in the \n",
    "    file_name list were all created for the same case (either case 2 or case 3 or case 4).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pandas.DataFrame\n",
    "        Data frame whose columns are \"Structure\" and \"False negative rate\" with one row per structure\n",
    "\n",
    "    '''\n",
    "    \n",
    "    file_names = glob(file_names)\n",
    "    file_names = sorted(file_names)\n",
    "    \n",
    "    # Use the subset adjacency matrix to create a dictionary\n",
    "    columns = np.array(subset.columns)\n",
    "    dictionary = dict(enumerate(columns.flatten(), 1))\n",
    "    dictionary = dict((value, key) for key, value in dictionary.items()) # Swap the keys and values\n",
    "    \n",
    "    # Create a counter for each structure\n",
    "    counters = [] # Empty list to be filled\n",
    "    for k in range(0, len(columns)): # For each structure in each repeat...\n",
    "        counters.append(0) # List of counters, one entry for each structure, each counter starts at 0\n",
    "    \n",
    "    # False negative rate calculation\n",
    "    for i in range(0, len(file_names)):\n",
    "        reject_p = np.zeros(len(columns)) # For each repeat, assume no structures are affected (null hypothesis)\n",
    "        file = np.load(file_names[i])\n",
    "        file = file[\"strings\"] # Values corresponding to the key\n",
    "        file = sorting_function(file, dictionary)\n",
    "    \n",
    "        for j in range(0, len(columns)): # For each structure in each repeat...\n",
    "            p = file[j].find(\"p=\") # Index of \"p\" in the string\n",
    "            pval = float(file[j][p::][2::]) # Extract the p-value\n",
    "            if pval < 0.05: # If the p-value is < 0.05...\n",
    "                reject_p[j] = 1\n",
    "            else: # The test statistic is the probabilities, which were sorted, so stop after the first structure we fail to reject\n",
    "                break\n",
    "        \n",
    "        # Calculate false negative rate the same for cases 3 or 4\n",
    "        if ((case == 3) or (case == 4)): # Go through every structure since both hippocampus and amygdala are affected\n",
    "            for k in range(0, len(columns)): # For each structure in each repeat...\n",
    "                for r, s in zip(reject_p, file):\n",
    "                    if columns[k] in s and r: # If a given structure name is in the string and we decided to reject H0...\n",
    "                        counters[k] += 1\n",
    "        \n",
    "        # Calculate false negative rate differently for case 2\n",
    "        elif case == 2:\n",
    "            for k in range(0, len(columns)): # For each structure in each repeat...\n",
    "                for r, s in zip(reject_p, file):\n",
    "                    if \"Amyg\" in s and r:\n",
    "                        continue # Don't consider the amygdala structures for case 2\n",
    "                    if columns[k] in s and r: # If a given structure name is in the string and we decided to reject H0...\n",
    "                        counters[k] += 1\n",
    "        \n",
    "    output = pd.DataFrame({\"Structure\": columns, \"False negative rate\": 1 - np.array(counters) / n_repeats})\n",
    "    \n",
    "    if case == 2:\n",
    "        output = output[~output[\"Structure\"].str.startswith(\"Amyg\")] # Omit the rows corresponding to amygdala structures\n",
    "    \n",
    "    return output                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5633c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

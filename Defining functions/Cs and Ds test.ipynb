{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c7b729",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "784a0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a17e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions\n",
    "\n",
    "from functions_py_file import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1840374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "\n",
    "adjacency_matrix = pd.read_csv(\"adjacency_matrix2.csv\", header=0, index_col=0)\n",
    "\n",
    "multilevel = pd.read_csv(\"multilevel2.csv\", header=0, index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03020fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the adjacency matrix\n",
    "adjacency_matrix = clean_adjacency_mat(adjacency_matrix)\n",
    "\n",
    "# Clean the multilevel lookup table\n",
    "multilevel = clean_multilevel(multilevel, adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d755a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the subset adjacency matrix\n",
    "\n",
    "subset_leaf_list = [\"Amyg_L_73_1\", \"Hippo_L_75_1\"]\n",
    "subset = subset_matrix_creator(subset_leaf_list, adjacency_matrix, multilevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "207c1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the descendants matrix\n",
    "descendants = adjacency_descendants(subset, N=20, mu=3.0)\n",
    "\n",
    "# Create the ancestors matrix\n",
    "ancestors = adjacency_ancestors(subset, N=20, mu=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47dc5a",
   "metadata": {},
   "source": [
    "## 2. Generate data for calculating Ds and Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf9225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated data\n",
    "\n",
    "generate_simulated_data(filename=\"test1_data.npz\", subset=subset, case=1, n_repeats=10, N=20, mu=3.0)\n",
    "generate_simulated_data(filename=\"test2_data.npz\", subset=subset, case=2, n_repeats=10, N=20, mu=3.0)\n",
    "generate_simulated_data(filename=\"test3_data.npz\", subset=subset, case=3, n_repeats=10, N=20, mu=3.0)\n",
    "generate_simulated_data(filename=\"test4_data.npz\", subset=subset, case=4, n_repeats=10, N=20, mu=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dcd892",
   "metadata": {},
   "source": [
    "## 3. Calculating Ds (our version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25255450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify our current permutation testing function to have outputs \n",
    "\n",
    "def permutation_testing(filename_old, filename_new, subset, n_repeats, nperm, N, mu, niter, clip, initial_prob):\n",
    "    ''' Function that conducts permutation testing\n",
    "    \n",
    "    This function conducts permutation testing using the generated data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename_old: string\n",
    "        The pattern of the filenames of the generated data; example: \"test1_data_repeat_*\"\n",
    "    \n",
    "    filename_new: string\n",
    "        The user-specified filename pattern for the permutation testing results (choose a different name from \n",
    "        filename_old, i.e. say \"results\" instead of \"data\" if you don't want generated data to get overwritten \n",
    "        by permutation testing results); example: \"results.npz\"\n",
    "    \n",
    "    subset: pandas.DataFrame\n",
    "        The subset adjacency matrix\n",
    "    \n",
    "    n_repeats: int\n",
    "        The number of repeats. We generated a random dataset with the same parameters but `n_repeats` \n",
    "        different realizations of the random variables. `n_repeats` must be the same value as `n_repeats`\n",
    "        when we generated data earlier. n_repeats should be 1 in practice for external users, but in our case, \n",
    "        since we simulated a lot of data, n_repeats is greater than 1.\n",
    "    \n",
    "    nperm: int\n",
    "        The number of permutations for permutation testing \n",
    "    \n",
    "    N: int\n",
    "        The number of samples. N must be the same value as N from generating data earlier.\n",
    "    \n",
    "    mu: float\n",
    "        The difference in means (generally unknown). mu should be the same value as mu from generating data\n",
    "        earlier in order to get meaningful results. But, mu doesn't have to be the same if you don't want to\n",
    "        make it the same.\n",
    "    \n",
    "    niter: int\n",
    "        The number of iterations of the EM algorithm\n",
    "    \n",
    "    clip: float\n",
    "        Number that clips probabilities away from 0 or 1\n",
    "    \n",
    "    initial_prob: float\n",
    "        The intial probability\n",
    "    \n",
    "    Returns \n",
    "    ----------\n",
    "    npz file (written to disk, not explicitly returned)\n",
    "        The 1st array contains p-values, 2nd array contains the names of the structures in the subset, 3rd \n",
    "        array contains the posterior probabilities, and 4th array contains the information from the prior 3 \n",
    "        arrays saved in 1 string per structure.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    M = subset.shape[0] # Number of total unique structures\n",
    "    S = np.array(subset, dtype = bool)\n",
    "    names_subset = subset.columns # List of the 8 structures' names\n",
    "    Descendants = adjacency_descendants(subset, N=N, mu=mu)\n",
    "    Descendants_and_self = np.logical_or(Descendants, np.eye(M))\n",
    "    \n",
    "    basename, extension = os.path.splitext(filename_new)\n",
    "    filename_old = glob(filename_old)\n",
    "    filename_old = sorted(filename_old)\n",
    "    \n",
    "    for j in range(n_repeats):\n",
    "        \n",
    "        # Load the generated data for each repeat\n",
    "        data = np.load(filename_old[j])\n",
    "        X = data[\"X\"]\n",
    "        Z = data[\"Z\"]\n",
    "        G = data[\"G\"]\n",
    "        \n",
    "        outputs = [] # Empty list for each iteration\n",
    "        \n",
    "        ### PARAMETER ESTIMATION ###\n",
    "    \n",
    "        P_subset = np.ones(M) * 0.5 # Array of 8 copies of 0.5\n",
    "        Q = Q_from_P(P_subset, S)\n",
    "\n",
    "        P0 = np.ones(M) * initial_prob\n",
    "        P_subset = estimate_P(X[G], mu, S, Descendants_and_self, draw=0, P0=P0, niter=niter, names=names_subset, clip=clip)\n",
    "        # Set draw = 0 to prevent drawing the graphs\n",
    "        \n",
    "        ### GENERATING PERMUTED DATA ###\n",
    "    \n",
    "        Ps = []\n",
    "        for n in range(nperm):\n",
    "            Xp = X[np.random.permutation(N)[G]]\n",
    "            P_ = estimate_P(Xp,mu,S,Descendants_and_self,draw=0,niter=niter,P0=P0)\n",
    "            Ps.append(P_)\n",
    "\n",
    "        Ps_sort = np.array([np.sort(Pi)[::-1] for Pi in Ps])\n",
    "        \n",
    "        ### PERMUTATION TESTING ###\n",
    "    \n",
    "        inds = np.argsort(P_subset)[::-1]\n",
    "        pval = np.zeros_like(P_subset)\n",
    "        alpha = 0.05\n",
    "        \n",
    "        pval_list = [] # Empty list to be filled\n",
    "        names_list = [] # Empty list to be filled\n",
    "        posterior_list = [] # Empty list to be filled\n",
    "        \n",
    "        for i in range(M):    \n",
    "            pval[inds[i]] = np.mean(Ps_sort[:,i] >= P_subset[inds[i]])\n",
    "            outputs.append(f\"{names_subset[inds[i]]}, P[Z=1|X]={P_subset[inds[i]]}, p={pval[inds[i]]}\")\n",
    "            # Every structure that gets rejected gets an entry\n",
    "            \n",
    "            pval_list.append(pval[inds[i]])\n",
    "            names_list.append(names_subset[inds[i]])\n",
    "            posterior_list.append(P_subset[inds[i]])\n",
    "        \n",
    "        ### SORT THE POSTERIOR VALUES ###\n",
    "        \n",
    "        # Use the subset adjacency matrix to create a dictionary\n",
    "        columns = np.array(subset.columns)\n",
    "        dictionary = dict(enumerate(columns.flatten(), 1))\n",
    "        dictionary = dict((value, key) for key, value in dictionary.items()) # Swap the keys and values\n",
    "        outputs = sorting_function(outputs, dictionary)\n",
    "        \n",
    "        ### SAVE DATA ### \n",
    "        \n",
    "        filename_new_this_repeat = basename + f'_repeat_{j:06d}' + extension\n",
    "        np.savez(filename_new_this_repeat, pval = pval_list, names = names_list, posterior = posterior_list, strings = outputs)\n",
    "        \n",
    "        ### OUTPUTS ###\n",
    "        \n",
    "        return np.quantile(Ps_sort, 0.95, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52bb49e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDs for test 1:\u001b[0m\n",
      "[0.06163906 0.06163906 0.06163906 0.06163906 0.00196239 0.00196239\n",
      " 0.00013929 0.00013929]\n",
      "\n",
      "\n",
      "\u001b[1mDs for test 2:\u001b[0m\n",
      "[0.74180851 0.74180851 0.74180851 0.74180851 0.67669326 0.67669326\n",
      " 0.01165132 0.01165132]\n",
      "\n",
      "\n",
      "\u001b[1mDs for test 3:\u001b[0m\n",
      "[0.81810099 0.81810099 0.81810099 0.81810099 0.7822012  0.7822012\n",
      " 0.74401794 0.74401794]\n",
      "\n",
      "\n",
      "\u001b[1mDs for test 4:\u001b[0m\n",
      "[0.75531041 0.75531041 0.75531041 0.75531041 0.6612898  0.6612898\n",
      " 0.01397258 0.01397258]\n"
     ]
    }
   ],
   "source": [
    "# Permutation testing\n",
    "\n",
    "print(\"\\033[1m\" + \"Ds for test 1:\" + \"\\033[0m\")\n",
    "print(permutation_testing(filename_old=\"test1_data_repeat_*\", filename_new = \"test1_results.npz\", subset=subset, n_repeats=10, nperm=10, N=20, mu=3.0, niter=5, initial_prob = 0.5, clip=0.001))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\033[1m\" + \"Ds for test 2:\" + \"\\033[0m\")\n",
    "print(permutation_testing(filename_old=\"test2_data_repeat_*\", filename_new = \"test2_results.npz\", subset=subset, n_repeats=10, nperm=10, N=20, mu=3.0, niter=5, clip=0.0001, initial_prob = 0.5))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\033[1m\" + \"Ds for test 3:\" + \"\\033[0m\")\n",
    "print(permutation_testing(filename_old=\"test3_data_repeat_*\", filename_new = \"test3_results.npz\", subset=subset, n_repeats=10, nperm=10, N=20, mu=3.0, niter=5, initial_prob = 0.25, clip=0.01))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\033[1m\" + \"Ds for test 4:\" + \"\\033[0m\")\n",
    "print(permutation_testing(filename_old=\"test4_data_repeat_*\", filename_new = \"test4_results.npz\", subset=subset, n_repeats=10, nperm=10, N=20, mu=3.0, niter=5, initial_prob = 0.75, clip=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc02c6b",
   "metadata": {},
   "source": [
    "## 4. Calculating Cs (based on the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea67f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified permutation testing function\n",
    "\n",
    "def permutation_testing2(filename_old, filename_new, subset, ignore, n_repeats, nperm, N, mu, niter, clip, initial_prob):\n",
    "    ''' Function that conducts permutation testing\n",
    "    \n",
    "    This function conducts permutation testing using the generated data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename_old: string\n",
    "        The pattern of the filenames of the generated data; example: \"test1_data_repeat_*\"\n",
    "    \n",
    "    filename_new: string\n",
    "        The user-specified filename pattern for the permutation testing results (choose a different name from \n",
    "        filename_old, i.e. say \"results\" instead of \"data\" if you don't want generated data to get overwritten \n",
    "        by permutation testing results); example: \"results.npz\"\n",
    "    \n",
    "    subset: pandas.DataFrame\n",
    "        The subset adjacency matrix\n",
    "    \n",
    "    ignore: list\n",
    "        A list of integers to ignore (structures that were rejected in previous function calls); \n",
    "        or it could be any empty list to not ignore anything\n",
    "    \n",
    "    n_repeats: int\n",
    "        The number of repeats. We generated a random dataset with the same parameters but `n_repeats` \n",
    "        different realizations of the random variables. `n_repeats` must be the same value as `n_repeats`\n",
    "        when we generated data earlier. n_repeats should be 1 in practice for external users, but in our case, \n",
    "        since we simulated a lot of data, n_repeats is greater than 1.\n",
    "    \n",
    "    nperm: int\n",
    "        The number of permutations for permutation testing \n",
    "    \n",
    "    N: int\n",
    "        The number of samples. N must be the same value as N from generating data earlier.\n",
    "    \n",
    "    mu: float\n",
    "        The difference in means (generally unknown). mu should be the same value as mu from generating data\n",
    "        earlier in order to get meaningful results. But, mu doesn't have to be the same if you don't want to\n",
    "        make it the same.\n",
    "    \n",
    "    niter: int\n",
    "        The number of iterations of the EM algorithm\n",
    "    \n",
    "    clip: float\n",
    "        Number that clips probabilities away from 0 or 1\n",
    "    \n",
    "    initial_prob: float\n",
    "        The intial probability\n",
    "    \n",
    "    Returns \n",
    "    ----------\n",
    "    npz file (written to disk, not explicitly returned)\n",
    "        The 1st array contains p-values, 2nd array contains the names of the structures in the subset, 3rd \n",
    "        array contains the posterior probabilities, and 4th array contains the information from the prior 3 \n",
    "        arrays saved in 1 string per structure.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    M = subset.shape[0] # Number of total unique structures\n",
    "    S = np.array(subset, dtype = bool)\n",
    "    names_subset = subset.columns # List of the 8 structures' names\n",
    "    Descendants = adjacency_descendants(subset, N=N, mu=mu)\n",
    "    Descendants_and_self = np.logical_or(Descendants, np.eye(M))\n",
    "    \n",
    "    basename, extension = os.path.splitext(filename_new)\n",
    "    filename_old = glob(filename_old)\n",
    "    filename_old = sorted(filename_old)\n",
    "    \n",
    "    for j in range(n_repeats):\n",
    "        \n",
    "        # Load the generated data for each repeat\n",
    "        data = np.load(filename_old[j])\n",
    "        X = data[\"X\"]\n",
    "        Z = data[\"Z\"]\n",
    "        G = data[\"G\"]\n",
    "        \n",
    "        outputs = [] # Empty list for each iteration\n",
    "        \n",
    "        ### PARAMETER ESTIMATION ###\n",
    "    \n",
    "        P_subset = np.ones(M) * 0.5 # Array of 8 copies of 0.5\n",
    "        Q = Q_from_P(P_subset, S)\n",
    "\n",
    "        P0 = np.ones(M) * initial_prob\n",
    "        P_subset = estimate_P(X[G], mu, S, Descendants_and_self, draw=0, P0=P0, niter=niter, names=names_subset, clip=clip)\n",
    "        # Set draw = 0 to prevent drawing the graphs\n",
    "        P_subset[ignore] = -1 # Set the ignored structures to -1 so they'll never be the maximum \n",
    "        ind = np.argmax(P_subset) # Index of the maximum value of P_subset (among the non-excluded structures)\n",
    "        \n",
    "        ### GENERATING PERMUTED DATA ###\n",
    "    \n",
    "        Ps = []\n",
    "        for n in range(nperm):\n",
    "            Xp = X[np.random.permutation(N)[G]]\n",
    "            P_ = estimate_P(Xp,mu,S,Descendants_and_self,draw=0,niter=niter,P0=P0)\n",
    "            P_ = [element for i, element in enumerate(P_) if i not in ignore] # Remove ignored structures from P_\n",
    "            Ps.append(P_) # Append the subset of P_\n",
    "        \n",
    "        Ps = np.array(Ps)\n",
    "        Ps_sort = np.max(Ps, axis = 1) # Maximum value of Ps\n",
    "        \n",
    "        ### PERMUTATION TESTING ###\n",
    "    \n",
    "        inds = np.argsort(P_subset)[::-1]\n",
    "        pval = np.zeros_like(P_subset)\n",
    "        alpha = 0.05\n",
    "        \n",
    "        pval_list = [] # Empty list to be filled\n",
    "        names_list = [] # Empty list to be filled\n",
    "        posterior_list = [] # Empty list to be filled\n",
    "        \n",
    "        for i in range(1):   # note i = 0\n",
    "            pval = np.mean(Ps_sort >= P_subset[inds[i]]) \n",
    "            outputs.append(f\"{names_subset[inds[i]]}, P[Z=1|X]={P_subset[inds[i]]}, p={pval}\")\n",
    "            # Every structure that gets rejected gets an entry\n",
    "            \n",
    "            names_list.append(names_subset[inds[i]])\n",
    "            posterior_list.append(P_subset[inds[i]])\n",
    "        \n",
    "        ### SORT THE POSTERIOR VALUES ###\n",
    "        \n",
    "        # Use the subset adjacency matrix to create a dictionary\n",
    "        columns = np.array(subset.columns)\n",
    "        dictionary = dict(enumerate(columns.flatten(), 1))\n",
    "        dictionary = dict((value, key) for key, value in dictionary.items()) # Swap the keys and values\n",
    "        outputs = sorting_function(outputs, dictionary)\n",
    "        \n",
    "        ### SAVE DATA ### \n",
    "        \n",
    "        filename_new_this_repeat = basename + f'_repeat_{j:06d}' + extension\n",
    "        np.savez(filename_new_this_repeat, pval = pval_list, names = names_list, posterior = posterior_list, strings = outputs)\n",
    "        \n",
    "        ### OUTPUTS ###\n",
    "        return pval, ind, np.quantile(Ps_sort, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1c33406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT THE STEPDOWN PROCEDURE\n",
    "\n",
    "ignore_list = [] # Initialize ignore_list as an empty list\n",
    "critical_values = [] # Initialize critical_values as an empty list\n",
    "\n",
    "while True: # Loop to call the function multiple times\n",
    "    results = permutation_testing2(filename_old=\"test1_data_repeat_*\", filename_new = \"test1_results.npz\", ignore = ignore_list, subset=subset, n_repeats=10, nperm=10, N=20, mu=3.0, niter=5, initial_prob = 0.5, clip=0.001)\n",
    "    if (results[0] < 0.05): # If the output p-value is < 0.05...\n",
    "        ignore_list.append(results[1]) # Add the maximum structure to the ignore list\n",
    "        critical_values.append(results[2])\n",
    "    else: # If not, then break out of the loop\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36669857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5699d2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04220020724545444, 0.06096244453761335, 0.06575499914838413]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critical_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab38221f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1, 0.06667679133668909)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutation_testing2(filename_old=\"test1_data_repeat_*\", filename_new = \"test1_results.npz\", ignore = [0], subset=subset, n_repeats=10, nperm=10, N=20, mu=3.0, niter=5, initial_prob = 0.5, clip=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507fe6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

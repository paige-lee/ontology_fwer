{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c7b729",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784a0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a17e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions\n",
    "\n",
    "from functions_py_file import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1840374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "\n",
    "adjacency_matrix = pd.read_csv(\"adjacency_matrix2.csv\", header=0, index_col=0)\n",
    "\n",
    "multilevel = pd.read_csv(\"multilevel2.csv\", header=0, index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03020fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the adjacency matrix\n",
    "adjacency_matrix = clean_adjacency_mat(adjacency_matrix)\n",
    "\n",
    "# Clean the multilevel lookup table\n",
    "multilevel = clean_multilevel(multilevel, adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d755a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the subset adjacency matrix\n",
    "\n",
    "subset_leaf_list = [\"Amyg_L_73_1\", \"Hippo_L_75_1\"]\n",
    "subset = subset_matrix_creator(subset_leaf_list, adjacency_matrix, multilevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "207c1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the descendants matrix\n",
    "descendants = adjacency_descendants(subset, N=20, mu=3.0)\n",
    "\n",
    "# Create the ancestors matrix\n",
    "ancestors = adjacency_ancestors(subset, N=20, mu=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47dc5a",
   "metadata": {},
   "source": [
    "## 2. Generate data for calculating Ds and Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf9225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated data\n",
    "\n",
    "np.random.seed(5)\n",
    "generate_simulated_data(filename=\"test1_data.npz\", subset=subset, case=1, n_repeats=1, N=20, mu=3.0)\n",
    "generate_simulated_data(filename=\"test2_data.npz\", subset=subset, case=2, n_repeats=1, N=20, mu=3.0)\n",
    "generate_simulated_data(filename=\"test3_data.npz\", subset=subset, case=3, n_repeats=1, N=20, mu=3.0)\n",
    "generate_simulated_data(filename=\"test4_data.npz\", subset=subset, case=4, n_repeats=1, N=20, mu=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dcd892",
   "metadata": {},
   "source": [
    "## 3. Calculating Ds (our version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25255450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify our current permutation testing function to have outputs \n",
    "\n",
    "def permutation_testing(filename_old, filename_new, subset, n_repeats, nperm, N, mu, niter, clip, initial_prob):\n",
    "    ''' Function that conducts permutation testing\n",
    "    \n",
    "    This function conducts permutation testing using the generated data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename_old: string\n",
    "        The pattern of the filenames of the generated data; example: \"test1_data_repeat_*\"\n",
    "    \n",
    "    filename_new: string\n",
    "        The user-specified filename pattern for the permutation testing results (choose a different name from \n",
    "        filename_old, i.e. say \"results\" instead of \"data\" if you don't want generated data to get overwritten \n",
    "        by permutation testing results); example: \"results.npz\"\n",
    "    \n",
    "    subset: pandas.DataFrame\n",
    "        The subset adjacency matrix\n",
    "    \n",
    "    n_repeats: int\n",
    "        The number of repeats. We generated a random dataset with the same parameters but `n_repeats` \n",
    "        different realizations of the random variables. `n_repeats` must be the same value as `n_repeats`\n",
    "        when we generated data earlier. n_repeats should be 1 in practice for external users, but in our case, \n",
    "        since we simulated a lot of data, n_repeats is greater than 1.\n",
    "    \n",
    "    nperm: int\n",
    "        The number of permutations for permutation testing \n",
    "    \n",
    "    N: int\n",
    "        The number of samples. N must be the same value as N from generating data earlier.\n",
    "    \n",
    "    mu: float\n",
    "        The difference in means (generally unknown). mu should be the same value as mu from generating data\n",
    "        earlier in order to get meaningful results. But, mu doesn't have to be the same if you don't want to\n",
    "        make it the same.\n",
    "    \n",
    "    niter: int\n",
    "        The number of iterations of the EM algorithm\n",
    "    \n",
    "    clip: float\n",
    "        Number that clips probabilities away from 0 or 1\n",
    "    \n",
    "    initial_prob: float\n",
    "        The intial probability\n",
    "    \n",
    "    Returns \n",
    "    ----------\n",
    "    npz file (written to disk, not explicitly returned)\n",
    "        The 1st array contains p-values, 2nd array contains the names of the structures in the subset, 3rd \n",
    "        array contains the posterior probabilities, and 4th array contains the information from the prior 3 \n",
    "        arrays saved in 1 string per structure.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    M = subset.shape[0] # Number of total unique structures\n",
    "    S = np.array(subset, dtype = bool)\n",
    "    names_subset = subset.columns # List of the 8 structures' names\n",
    "    Descendants = adjacency_descendants(subset, N=N, mu=mu)\n",
    "    Descendants_and_self = np.logical_or(Descendants, np.eye(M))\n",
    "    \n",
    "    basename, extension = os.path.splitext(filename_new)\n",
    "    filename_old = glob(filename_old)\n",
    "    filename_old = sorted(filename_old)\n",
    "    \n",
    "    for j in range(n_repeats):\n",
    "        \n",
    "        # Load the generated data for each repeat\n",
    "        data = np.load(filename_old[j])\n",
    "        X = data[\"X\"]\n",
    "        Z = data[\"Z\"]\n",
    "        G = data[\"G\"]\n",
    "        \n",
    "        outputs = [] # Empty list for each iteration\n",
    "        \n",
    "        ### PARAMETER ESTIMATION ###\n",
    "    \n",
    "        P_subset = np.ones(M) * 0.5 # Array of 8 copies of 0.5\n",
    "        Q = Q_from_P(P_subset, S)\n",
    "\n",
    "        P0 = np.ones(M) * initial_prob\n",
    "        P_subset = estimate_P(X[G], mu, S, Descendants_and_self, draw=0, P0=P0, niter=niter, names=names_subset, clip=clip)\n",
    "        # Set draw = 0 to prevent drawing the graphs\n",
    "        \n",
    "        ### GENERATING PERMUTED DATA ###\n",
    "    \n",
    "        Ps = []\n",
    "        np.random.seed(5)\n",
    "        for n in range(nperm):\n",
    "            Xp = X[np.random.permutation(N)[G]]\n",
    "            P_ = estimate_P(Xp,mu,S,Descendants_and_self,draw=0,niter=niter,P0=P0)\n",
    "            Ps.append(P_)\n",
    "\n",
    "        Ps_sort = np.array([np.sort(Pi)[::-1] for Pi in Ps])\n",
    "        \n",
    "        \n",
    "        ### PERMUTATION TESTING ###\n",
    "    \n",
    "        inds = np.argsort(P_subset)[::-1]\n",
    "        pval = np.zeros_like(P_subset)\n",
    "        alpha = 0.05\n",
    "        \n",
    "        pval_list = [] # Empty list to be filled\n",
    "        names_list = [] # Empty list to be filled\n",
    "        posterior_list = [] # Empty list to be filled\n",
    "        \n",
    "        for i in range(M):    \n",
    "            pval[inds[i]] = np.mean(Ps_sort[:,i] >= P_subset[inds[i]])\n",
    "            outputs.append(f\"{names_subset[inds[i]]}, P[Z=1|X]={P_subset[inds[i]]}, p={pval[inds[i]]}\")\n",
    "            # Every structure that gets rejected gets an entry\n",
    "            \n",
    "            pval_list.append(pval[inds[i]])\n",
    "            names_list.append(names_subset[inds[i]])\n",
    "            posterior_list.append(P_subset[inds[i]])\n",
    "        \n",
    "        ### SORT THE POSTERIOR VALUES ###\n",
    "        \n",
    "        # Use the subset adjacency matrix to create a dictionary\n",
    "        columns = np.array(subset.columns)\n",
    "        dictionary = dict(enumerate(columns.flatten(), 1))\n",
    "        dictionary = dict((value, key) for key, value in dictionary.items()) # Swap the keys and values\n",
    "        outputs = sorting_function(outputs, dictionary)\n",
    "        \n",
    "        ### SAVE DATA ### \n",
    "        \n",
    "        filename_new_this_repeat = basename + f'_repeat_{j:06d}' + extension\n",
    "        np.savez(filename_new_this_repeat, pval = pval_list, names = names_list, posterior = posterior_list, strings = outputs)\n",
    "        \n",
    "    ### OUTPUTS ###\n",
    "    return pval_list, inds, np.quantile(Ps_sort, 0.95, axis=0), names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52bb49e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDs for test 1:\u001b[0m\n",
      "([0.13, 0.13, 0.13, 0.13, 0.16, 0.16, 0.08, 0.08], array([3, 2, 1, 0, 6, 4, 7, 5]), array([0.18009747, 0.18009747, 0.18009747, 0.18009747, 0.05389237,\n",
      "       0.05389237, 0.00808257, 0.00808257]), ['Limbic_L_434_3', 'CerebralCortex_L_482_4', 'Telencephalon_L_501_5', 'Everything', 'Hippo_L_75_1', 'Hippo_L_338_2', 'Amyg_L_73_1', 'Amyg_L_336_2'])\n",
      "\n",
      "\n",
      "\u001b[1mDs for test 2:\u001b[0m\n",
      "([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.19], array([3, 2, 1, 0, 6, 4, 7, 5]), array([0.79965857, 0.79965857, 0.79965857, 0.79965857, 0.75869129,\n",
      "       0.75869129, 0.04938677, 0.04938677]), ['Limbic_L_434_3', 'CerebralCortex_L_482_4', 'Telencephalon_L_501_5', 'Everything', 'Hippo_L_75_1', 'Hippo_L_338_2', 'Amyg_L_73_1', 'Amyg_L_336_2'])\n",
      "\n",
      "\n",
      "\u001b[1mDs for test 3:\u001b[0m\n",
      "([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], array([3, 2, 1, 0, 7, 5, 6, 4]), array([0.83026973, 0.83026973, 0.83026973, 0.83026973, 0.82089097,\n",
      "       0.82089097, 0.70813785, 0.70813785]), ['Limbic_L_434_3', 'CerebralCortex_L_482_4', 'Telencephalon_L_501_5', 'Everything', 'Amyg_L_73_1', 'Amyg_L_336_2', 'Hippo_L_75_1', 'Hippo_L_338_2'])\n",
      "\n",
      "\n",
      "\u001b[1mDs for test 4:\u001b[0m\n",
      "([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], array([3, 2, 1, 0, 6, 4, 7, 5]), array([0.88232844, 0.88232844, 0.88232844, 0.88232844, 0.50226236,\n",
      "       0.50226236, 0.32022196, 0.32022196]), ['Limbic_L_434_3', 'CerebralCortex_L_482_4', 'Telencephalon_L_501_5', 'Everything', 'Hippo_L_75_1', 'Hippo_L_338_2', 'Amyg_L_73_1', 'Amyg_L_336_2'])\n"
     ]
    }
   ],
   "source": [
    "# Permutation testing\n",
    "\n",
    "print(\"\\033[1m\" + \"Ds for test 1:\" + \"\\033[0m\")\n",
    "print(permutation_testing(filename_old=\"test1_data_repeat_*\", filename_new = \"test1_results.npz\", subset=subset, n_repeats=1, nperm=100, N=20, mu=3.0, niter=5, initial_prob = 0.5, clip=0.001))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\033[1m\" + \"Ds for test 2:\" + \"\\033[0m\")\n",
    "print(permutation_testing(filename_old=\"test2_data_repeat_*\", filename_new = \"test2_results.npz\", subset=subset, n_repeats=1, nperm=100, N=20, mu=3.0, niter=5, clip=0.0001, initial_prob = 0.5))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\033[1m\" + \"Ds for test 3:\" + \"\\033[0m\")\n",
    "print(permutation_testing(filename_old=\"test3_data_repeat_*\", filename_new = \"test3_results.npz\", subset=subset, n_repeats=1, nperm=100, N=20, mu=3.0, niter=5, initial_prob = 0.25, clip=0.01))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"\\033[1m\" + \"Ds for test 4:\" + \"\\033[0m\")\n",
    "print(permutation_testing(filename_old=\"test4_data_repeat_*\", filename_new = \"test4_results.npz\", subset=subset, n_repeats=1, nperm=100, N=20, mu=3.0, niter=5, initial_prob = 0.75, clip=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc02c6b",
   "metadata": {},
   "source": [
    "## 4. Calculating Cs (based on the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea67f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified permutation testing function\n",
    "\n",
    "def permutation_testing2(filename_old, filename_new, subset, ignore, n_repeats, nperm, N, mu, niter, clip, initial_prob):\n",
    "    ''' Function that conducts permutation testing\n",
    "    \n",
    "    This function conducts permutation testing using the generated data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename_old: string\n",
    "        The pattern of the filenames of the generated data; example: \"test1_data_repeat_*\"\n",
    "    \n",
    "    filename_new: string\n",
    "        The user-specified filename pattern for the permutation testing results (choose a different name from \n",
    "        filename_old, i.e. say \"results\" instead of \"data\" if you don't want generated data to get overwritten \n",
    "        by permutation testing results); example: \"results.npz\"\n",
    "    \n",
    "    subset: pandas.DataFrame\n",
    "        The subset adjacency matrix\n",
    "    \n",
    "    ignore: list\n",
    "        A list of integers to ignore (structures that were rejected in previous function calls); \n",
    "        or it could be any empty list to not ignore anything\n",
    "    \n",
    "    n_repeats: int\n",
    "        The number of repeats. We generated a random dataset with the same parameters but `n_repeats` \n",
    "        different realizations of the random variables. `n_repeats` must be the same value as `n_repeats`\n",
    "        when we generated data earlier. n_repeats should be 1 in practice for external users, but in our case, \n",
    "        since we simulated a lot of data, n_repeats is greater than 1.\n",
    "    \n",
    "    nperm: int\n",
    "        The number of permutations for permutation testing \n",
    "    \n",
    "    N: int\n",
    "        The number of samples. N must be the same value as N from generating data earlier.\n",
    "    \n",
    "    mu: float\n",
    "        The difference in means (generally unknown). mu should be the same value as mu from generating data\n",
    "        earlier in order to get meaningful results. But, mu doesn't have to be the same if you don't want to\n",
    "        make it the same.\n",
    "    \n",
    "    niter: int\n",
    "        The number of iterations of the EM algorithm\n",
    "    \n",
    "    clip: float\n",
    "        Number that clips probabilities away from 0 or 1\n",
    "    \n",
    "    initial_prob: float\n",
    "        The intial probability\n",
    "    \n",
    "    Returns \n",
    "    ----------\n",
    "    npz file (written to disk, not explicitly returned)\n",
    "        The 1st array contains p-values, 2nd array contains the names of the structures in the subset, 3rd \n",
    "        array contains the posterior probabilities, and 4th array contains the information from the prior 3 \n",
    "        arrays saved in 1 string per structure.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    M = subset.shape[0] # Number of total unique structures\n",
    "    S = np.array(subset, dtype = bool)\n",
    "    names_subset = subset.columns # List of the 8 structures' names\n",
    "    Descendants = adjacency_descendants(subset, N=N, mu=mu)\n",
    "    Descendants_and_self = np.logical_or(Descendants, np.eye(M))\n",
    "    \n",
    "    basename, extension = os.path.splitext(filename_new)\n",
    "    filename_old = glob(filename_old)\n",
    "    filename_old = sorted(filename_old)\n",
    "    \n",
    "    for j in range(n_repeats):\n",
    "        \n",
    "        # Load the generated data for each repeat\n",
    "        data = np.load(filename_old[j])\n",
    "        X = data[\"X\"]\n",
    "        Z = data[\"Z\"]\n",
    "        G = data[\"G\"]\n",
    "        \n",
    "        outputs = [] # Empty list for each iteration\n",
    "        \n",
    "        ### PARAMETER ESTIMATION ###\n",
    "    \n",
    "        P_subset = np.ones(M) * 0.5 # Array of 8 copies of 0.5\n",
    "        Q = Q_from_P(P_subset, S)\n",
    "\n",
    "        P0 = np.ones(M) * initial_prob\n",
    "        P_subset = estimate_P(X[G], mu, S, Descendants_and_self, draw=0, P0=P0, niter=niter, names=names_subset, clip=clip)\n",
    "        # Set draw = 0 to prevent drawing the graphs\n",
    "        P_subset[ignore] = -1 # Set the ignored structures to -1 so they'll never be the maximum \n",
    "        ind = np.argsort(P_subset)[::-1][0] # Index of the maximum value of P_subset (among the non-excluded structures)\n",
    "        \n",
    "        ### GENERATING PERMUTED DATA ###\n",
    "    \n",
    "        Ps = []\n",
    "        np.random.seed(5)\n",
    "        for n in range(nperm):\n",
    "            Xp = X[np.random.permutation(N)[G]]\n",
    "            P_ = estimate_P(Xp,mu,S,Descendants_and_self,draw=0,niter=niter,P0=P0)\n",
    "            P_ = [element for i, element in enumerate(P_) if i not in ignore] # Remove ignored structures from P_\n",
    "            Ps.append(P_) # Append the subset of P_\n",
    "        \n",
    "        Ps = np.array(Ps)\n",
    "        Ps_sort = np.max(Ps, axis = 1) # Maximum value of Ps\n",
    "        \n",
    "        ### PERMUTATION TESTING ###\n",
    "    \n",
    "        pval = np.zeros_like(P_subset)\n",
    "        alpha = 0.05\n",
    "        \n",
    "        pval_list = [] # Empty list to be filled\n",
    "        names_list = [] # Empty list to be filled\n",
    "        posterior_list = [] # Empty list to be filled\n",
    "        \n",
    "        pval = np.mean(Ps_sort >= P_subset[ind])\n",
    "        outputs.append(f\"{names_subset[ind]}, P[Z=1|X]={P_subset[ind]}, p={pval}\") \n",
    "        # Every structure that gets rejected gets an entry\n",
    "        names_list.append(names_subset[ind])\n",
    "        posterior_list.append(P_subset[ind])\n",
    "                    \n",
    "        \n",
    "        ### SORT THE POSTERIOR VALUES ###\n",
    "        \n",
    "        # Use the subset adjacency matrix to create a dictionary\n",
    "        columns = np.array(subset.columns)\n",
    "        dictionary = dict(enumerate(columns.flatten(), 1))\n",
    "        dictionary = dict((value, key) for key, value in dictionary.items()) # Swap the keys and values\n",
    "        outputs = sorting_function(outputs, dictionary)\n",
    "        \n",
    "        ### SAVE DATA ### \n",
    "        \n",
    "        filename_new_this_repeat = basename + f'_repeat_{j:06d}' + extension\n",
    "        np.savez(filename_new_this_repeat, pval = pval_list, names = names_list, posterior = posterior_list, strings = outputs)\n",
    "        \n",
    "    ### OUTPUTS ###\n",
    "    return pval, ind, np.quantile(Ps_sort, 0.95), names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1c33406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure:  ['Limbic_L_434_3']\n",
      "Structure:  ['CerebralCortex_L_482_4']\n",
      "Structure:  ['Telencephalon_L_501_5']\n",
      "Structure:  ['Everything']\n",
      "Structure:  ['Hippo_L_75_1']\n",
      "Structure:  ['Hippo_L_338_2']\n",
      "Structure:  ['Amyg_L_73_1']\n",
      "Structure:  ['Amyg_L_336_2']\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENT THE STEPDOWN PROCEDURE (case 1)\n",
    "\n",
    "ignore_list = [] # Initialize ignore_list as an empty list\n",
    "critical_values = [] # Initialize critical_values as an empty list\n",
    "p_values = [] # Initialize p_values as an empty list\n",
    "M = subset.shape[0] # Number of total unique structures\n",
    "\n",
    "while True: # Loop to call the function multiple times\n",
    "    results = permutation_testing2(filename_old=\"test1_data_repeat_*\", filename_new = \"test1_results.npz\", ignore = ignore_list, subset=subset, n_repeats=1, nperm=100, N=20, mu=3.0, niter=5, initial_prob = 0.5, clip=0.001)\n",
    "    print(\"Structure: \", results[3])\n",
    "    if ((results[0] < 0.05) or True): # If the output p-value is < 0.05...\n",
    "        ignore_list.append(results[1]) # Add the maximum structure to the ignore list\n",
    "        critical_values.append(results[2])\n",
    "        p_values.append(results[0])\n",
    "    else:\n",
    "        # if we fail to reject one hypothesis, we stop testing \n",
    "        # but, for our study we will still want all M structures, so above we say \"or True\"\n",
    "        break\n",
    "    if (len(ignore_list) == M): # Break once the ignore list is filled with every structure\n",
    "        break\n",
    "\n",
    "# Note: the break condition for case 1 must be different from the break condition for \n",
    "# the other cases since case 1 doesn't reject any structures (nothing is affected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36669857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1, 0, 6, 4, 7, 5]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5699d2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18009747162498949,\n",
       " 0.18009747162498949,\n",
       " 0.18009747162498949,\n",
       " 0.18009747162498949,\n",
       " 0.05389237178253237,\n",
       " 0.05389237178253237,\n",
       " 0.0143978837064615,\n",
       " 0.0143978837064615]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critical_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5843ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure:  ['Limbic_L_434_3']\n",
      "Structure:  ['CerebralCortex_L_482_4']\n",
      "Structure:  ['Telencephalon_L_501_5']\n",
      "Structure:  ['Everything']\n",
      "Structure:  ['Hippo_L_75_1']\n",
      "Structure:  ['Hippo_L_338_2']\n",
      "Structure:  ['Amyg_L_73_1']\n",
      "Structure:  ['Amyg_L_336_2']\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENT THE STEPDOWN PROCEDURE (case 2)\n",
    "\n",
    "ignore_list = [] # Initialize ignore_list as an empty list\n",
    "critical_values = [] # Initialize critical_values as an empty list\n",
    "p_values = [] # Initialize p_values as an empty list\n",
    "M = subset.shape[0] # Number of total unique structures\n",
    "\n",
    "while True: # Loop to call the function multiple times\n",
    "    results = permutation_testing2(filename_old=\"test2_data_repeat_*\", filename_new = \"test2_results.npz\", ignore = ignore_list, subset=subset, n_repeats=1, nperm=100, N=20, mu=3.0, niter=5, initial_prob = 0.5, clip=0.001)\n",
    "    print(\"Structure: \", results[3])\n",
    "    if ((results[0] < 0.05) or True): # If the output p-value is < 0.05...\n",
    "        ignore_list.append(results[1]) # Add the maximum structure to the ignore list\n",
    "        critical_values.append(results[2])\n",
    "        p_values.append(results[0])\n",
    "    else:\n",
    "        # if we fail to reject one hypothesis, we stop testing \n",
    "        # but, for our study we will still want all M structures, so above we say \"or True\"\n",
    "        break\n",
    "    if (len(ignore_list) == M): # Break once the ignore list is filled with every structure\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a620d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1, 0, 6, 4, 7, 5]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55833423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7996585683410531,\n",
       " 0.7996585683410531,\n",
       " 0.7996585683410531,\n",
       " 0.7996585683410531,\n",
       " 0.7586912866403912,\n",
       " 0.7586912866403912,\n",
       " 0.049386765210812575,\n",
       " 0.049386765210812575]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critical_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ddb4469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure:  ['Limbic_L_434_3']\n",
      "Structure:  ['CerebralCortex_L_482_4']\n",
      "Structure:  ['Telencephalon_L_501_5']\n",
      "Structure:  ['Everything']\n",
      "Structure:  ['Amyg_L_73_1']\n",
      "Structure:  ['Amyg_L_336_2']\n",
      "Structure:  ['Hippo_L_75_1']\n",
      "Structure:  ['Hippo_L_338_2']\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENT THE STEPDOWN PROCEDURE (case 3)\n",
    "\n",
    "ignore_list = [] # Initialize ignore_list as an empty list\n",
    "critical_values = [] # Initialize critical_values as an empty list\n",
    "p_values = [] # Initialize p_values as an empty list\n",
    "M = subset.shape[0] # Number of total unique structures\n",
    "\n",
    "while True: # Loop to call the function multiple times\n",
    "    results = permutation_testing2(filename_old=\"test3_data_repeat_*\", filename_new = \"test3_results.npz\", ignore = ignore_list, subset=subset, n_repeats=1, nperm=100, N=20, mu=3.0, niter=5, initial_prob = 0.5, clip=0.001)\n",
    "    print(\"Structure: \", results[3])\n",
    "    if ((results[0] < 0.05) or True): # If the output p-value is < 0.05...\n",
    "        ignore_list.append(results[1]) # Add the maximum structure to the ignore list\n",
    "        critical_values.append(results[2])\n",
    "        p_values.append(results[0])\n",
    "    else:\n",
    "        # if we fail to reject one hypothesis, we stop testing \n",
    "        # but, for our study we will still want all M structures, so above we say \"or True\"\n",
    "        break\n",
    "    if (len(ignore_list) == M): # Break once the ignore list is filled with every structure\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d951c1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1, 0, 7, 5, 6, 4]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f065440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8302206310591623,\n",
       " 0.8302206310591623,\n",
       " 0.8302206310591623,\n",
       " 0.8302206310591623,\n",
       " 0.8209352880796729,\n",
       " 0.8209352880796729,\n",
       " 0.7081589632500855,\n",
       " 0.7081589632500855]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critical_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4507fe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure:  ['Limbic_L_434_3']\n",
      "Structure:  ['CerebralCortex_L_482_4']\n",
      "Structure:  ['Telencephalon_L_501_5']\n",
      "Structure:  ['Everything']\n",
      "Structure:  ['Hippo_L_75_1']\n",
      "Structure:  ['Hippo_L_338_2']\n",
      "Structure:  ['Amyg_L_73_1']\n",
      "Structure:  ['Amyg_L_336_2']\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENT THE STEPDOWN PROCEDURE (case 4)\n",
    "\n",
    "ignore_list = [] # Initialize ignore_list as an empty list\n",
    "critical_values = [] # Initialize critical_values as an empty list\n",
    "p_values = [] # Initialize p_values as an empty list\n",
    "M = subset.shape[0] # Number of total unique structures\n",
    "\n",
    "while True: # Loop to call the function multiple times\n",
    "    results = permutation_testing2(filename_old=\"test4_data_repeat_*\", filename_new = \"test4_results.npz\", ignore = ignore_list, subset=subset, n_repeats=1, nperm=100, N=20, mu=3.0, niter=5, initial_prob = 0.5, clip=0.001)\n",
    "    print(\"Structure: \", results[3])\n",
    "    if ((results[0] < 0.05) or True): # If the output p-value is < 0.05...\n",
    "        ignore_list.append(results[1]) # Add the maximum structure to the ignore list\n",
    "        critical_values.append(results[2])\n",
    "        p_values.append(results[0])\n",
    "    else:\n",
    "        # if we fail to reject one hypothesis, we stop testing \n",
    "        # but, for our study we will still want all M structures, so above we say \"or True\"\n",
    "        break\n",
    "    if (len(ignore_list) == M): # Break once the ignore list is filled with every structure\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "985a82c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1, 0, 6, 4, 7, 5]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9601c0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8806514168387883,\n",
       " 0.8806514168387883,\n",
       " 0.8806514168387883,\n",
       " 0.8806514168387883,\n",
       " 0.5011948724854477,\n",
       " 0.5011948724854477,\n",
       " 0.3293905562318394,\n",
       " 0.3293905562318394]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critical_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f53bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

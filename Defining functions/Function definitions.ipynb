{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ce0f91",
   "metadata": {},
   "source": [
    "## 1. Import libraries, load data, clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ab59a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92620862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "\n",
    "adjacency_matrix = pd.read_csv(\"adjacency_matrix2.csv\", header=0, index_col=0)\n",
    "\n",
    "multilevel = pd.read_csv(\"multilevel2.csv\", header=0, index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d445046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Everything</th>\n",
       "      <th>CSF_508_5</th>\n",
       "      <th>Myelencephalon_507_5</th>\n",
       "      <th>Metencephalon_506_5</th>\n",
       "      <th>Mesencephalon_505_5</th>\n",
       "      <th>Diencephalon_R_504_5</th>\n",
       "      <th>Diencephalon_L_503_5</th>\n",
       "      <th>Telencephalon_R_502_5</th>\n",
       "      <th>Telencephalon_L_501_5</th>\n",
       "      <th>Sulcus_R_500_4</th>\n",
       "      <th>...</th>\n",
       "      <th>MFG_DPFC_R_10_1</th>\n",
       "      <th>MFG_DPFC_L_9_1</th>\n",
       "      <th>MFG_R_8_1</th>\n",
       "      <th>MFG_L_7_1</th>\n",
       "      <th>SFG_pole_R_6_1</th>\n",
       "      <th>SFG_pole_L_5_1</th>\n",
       "      <th>SFG_PFC_R_4_1</th>\n",
       "      <th>SFG_PFC_L_3_1</th>\n",
       "      <th>SFG_R_2_1</th>\n",
       "      <th>SFG_L_1_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Everything</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSF_508_5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Myelencephalon_507_5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metencephalon_506_5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mesencephalon_505_5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SFG_pole_L_5_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SFG_PFC_R_4_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SFG_PFC_L_3_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SFG_R_2_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SFG_L_1_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509 rows × 509 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Everything  CSF_508_5  Myelencephalon_507_5  \\\n",
       "Everything                     0          1                     1   \n",
       "CSF_508_5                      0          0                     0   \n",
       "Myelencephalon_507_5           0          0                     0   \n",
       "Metencephalon_506_5            0          0                     0   \n",
       "Mesencephalon_505_5            0          0                     0   \n",
       "...                          ...        ...                   ...   \n",
       "SFG_pole_L_5_1                 0          0                     0   \n",
       "SFG_PFC_R_4_1                  0          0                     0   \n",
       "SFG_PFC_L_3_1                  0          0                     0   \n",
       "SFG_R_2_1                      0          0                     0   \n",
       "SFG_L_1_1                      0          0                     0   \n",
       "\n",
       "                      Metencephalon_506_5  Mesencephalon_505_5  \\\n",
       "Everything                              1                    1   \n",
       "CSF_508_5                               0                    0   \n",
       "Myelencephalon_507_5                    0                    0   \n",
       "Metencephalon_506_5                     0                    0   \n",
       "Mesencephalon_505_5                     0                    0   \n",
       "...                                   ...                  ...   \n",
       "SFG_pole_L_5_1                          0                    0   \n",
       "SFG_PFC_R_4_1                           0                    0   \n",
       "SFG_PFC_L_3_1                           0                    0   \n",
       "SFG_R_2_1                               0                    0   \n",
       "SFG_L_1_1                               0                    0   \n",
       "\n",
       "                      Diencephalon_R_504_5  Diencephalon_L_503_5  \\\n",
       "Everything                               1                     1   \n",
       "CSF_508_5                                0                     0   \n",
       "Myelencephalon_507_5                     0                     0   \n",
       "Metencephalon_506_5                      0                     0   \n",
       "Mesencephalon_505_5                      0                     0   \n",
       "...                                    ...                   ...   \n",
       "SFG_pole_L_5_1                           0                     0   \n",
       "SFG_PFC_R_4_1                            0                     0   \n",
       "SFG_PFC_L_3_1                            0                     0   \n",
       "SFG_R_2_1                                0                     0   \n",
       "SFG_L_1_1                                0                     0   \n",
       "\n",
       "                      Telencephalon_R_502_5  Telencephalon_L_501_5  \\\n",
       "Everything                                1                      1   \n",
       "CSF_508_5                                 0                      0   \n",
       "Myelencephalon_507_5                      0                      0   \n",
       "Metencephalon_506_5                       0                      0   \n",
       "Mesencephalon_505_5                       0                      0   \n",
       "...                                     ...                    ...   \n",
       "SFG_pole_L_5_1                            0                      0   \n",
       "SFG_PFC_R_4_1                             0                      0   \n",
       "SFG_PFC_L_3_1                             0                      0   \n",
       "SFG_R_2_1                                 0                      0   \n",
       "SFG_L_1_1                                 0                      0   \n",
       "\n",
       "                      Sulcus_R_500_4  ...  MFG_DPFC_R_10_1  MFG_DPFC_L_9_1  \\\n",
       "Everything                         0  ...                0               0   \n",
       "CSF_508_5                          1  ...                0               0   \n",
       "Myelencephalon_507_5               0  ...                0               0   \n",
       "Metencephalon_506_5                0  ...                0               0   \n",
       "Mesencephalon_505_5                0  ...                0               0   \n",
       "...                              ...  ...              ...             ...   \n",
       "SFG_pole_L_5_1                     0  ...                0               0   \n",
       "SFG_PFC_R_4_1                      0  ...                0               0   \n",
       "SFG_PFC_L_3_1                      0  ...                0               0   \n",
       "SFG_R_2_1                          0  ...                0               0   \n",
       "SFG_L_1_1                          0  ...                0               0   \n",
       "\n",
       "                      MFG_R_8_1  MFG_L_7_1  SFG_pole_R_6_1  SFG_pole_L_5_1  \\\n",
       "Everything                    0          0               0               0   \n",
       "CSF_508_5                     0          0               0               0   \n",
       "Myelencephalon_507_5          0          0               0               0   \n",
       "Metencephalon_506_5           0          0               0               0   \n",
       "Mesencephalon_505_5           0          0               0               0   \n",
       "...                         ...        ...             ...             ...   \n",
       "SFG_pole_L_5_1                0          0               0               0   \n",
       "SFG_PFC_R_4_1                 0          0               0               0   \n",
       "SFG_PFC_L_3_1                 0          0               0               0   \n",
       "SFG_R_2_1                     0          0               0               0   \n",
       "SFG_L_1_1                     0          0               0               0   \n",
       "\n",
       "                      SFG_PFC_R_4_1  SFG_PFC_L_3_1  SFG_R_2_1  SFG_L_1_1  \n",
       "Everything                        0              0          0          0  \n",
       "CSF_508_5                         0              0          0          0  \n",
       "Myelencephalon_507_5              0              0          0          0  \n",
       "Metencephalon_506_5               0              0          0          0  \n",
       "Mesencephalon_505_5               0              0          0          0  \n",
       "...                             ...            ...        ...        ...  \n",
       "SFG_pole_L_5_1                    0              0          0          0  \n",
       "SFG_PFC_R_4_1                     0              0          0          0  \n",
       "SFG_PFC_L_3_1                     0              0          0          0  \n",
       "SFG_R_2_1                         0              0          0          0  \n",
       "SFG_L_1_1                         0              0          0          0  \n",
       "\n",
       "[509 rows x 509 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the adjacency matrix\n",
    "\n",
    "def clean_adjacency_mat(adjacency_matrix):\n",
    "    ''' Function that cleans the adjacency matrix \n",
    "    \n",
    "    This function cleans the raw adjacency matrix so that parents come before children. \n",
    "    This converts between one convention and another convention. It doesn't check for parents vs. children.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix: pandas.DataFrame\n",
    "        A square adjacency matrix, where 1 in the (i, j) entry means the ith structure is a parent of the \n",
    "        jth structure.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pandas.DataFrame\n",
    "        A cleaned adjacency matrix (parents come before children)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Reverse the order of columns \n",
    "    columns = adjacency_matrix.columns.tolist()\n",
    "    columns = columns[::-1]\n",
    "    adjacency_matrix = adjacency_matrix[columns]\n",
    "    \n",
    "    # Reverse the order of rows\n",
    "    adjacency_matrix = adjacency_matrix[::-1]\n",
    "    \n",
    "    # Take the transpose of the matrix\n",
    "    adjacency_matrix = adjacency_matrix.T\n",
    "    \n",
    "    return adjacency_matrix\n",
    "\n",
    "adjacency_matrix = clean_adjacency_mat(adjacency_matrix)\n",
    "adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39f22d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Structure</th>\n",
       "      <th>Immediate.parent</th>\n",
       "      <th>Immediate.child.children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>1</td>\n",
       "      <td>Everything</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Telencephalon_L_501_5, Telencephalon_R_502_5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>2</td>\n",
       "      <td>CSF_508_5</td>\n",
       "      <td>Everything</td>\n",
       "      <td>Ventricle_498_4, Sulcus_L_499_4, Sulcus_R_500_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>3</td>\n",
       "      <td>Myelencephalon_507_5</td>\n",
       "      <td>Everything</td>\n",
       "      <td>Myelencephalon_L_494_4, Myelencephalon_R_495_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>4</td>\n",
       "      <td>Metencephalon_506_5</td>\n",
       "      <td>Everything</td>\n",
       "      <td>Metencephalon_L_492_4, Metencephalon_R_493_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>5</td>\n",
       "      <td>Mesencephalon_505_5</td>\n",
       "      <td>Everything</td>\n",
       "      <td>Mesencephalon_L_490_4, Mesencephalon_R_491_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>505</td>\n",
       "      <td>SFG_pole_L_5_1</td>\n",
       "      <td>SFG_L_290_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>506</td>\n",
       "      <td>SFG_PFC_R_4_1</td>\n",
       "      <td>SFG_R_291_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>507</td>\n",
       "      <td>SFG_PFC_L_3_1</td>\n",
       "      <td>SFG_L_290_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>508</td>\n",
       "      <td>SFG_R_2_1</td>\n",
       "      <td>SFG_R_291_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>509</td>\n",
       "      <td>SFG_L_1_1</td>\n",
       "      <td>SFG_L_290_2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Number             Structure Immediate.parent  \\\n",
       "509       1            Everything              NaN   \n",
       "508       2             CSF_508_5       Everything   \n",
       "507       3  Myelencephalon_507_5       Everything   \n",
       "506       4   Metencephalon_506_5       Everything   \n",
       "505       5   Mesencephalon_505_5       Everything   \n",
       "..      ...                   ...              ...   \n",
       "5       505        SFG_pole_L_5_1      SFG_L_290_2   \n",
       "4       506         SFG_PFC_R_4_1      SFG_R_291_2   \n",
       "3       507         SFG_PFC_L_3_1      SFG_L_290_2   \n",
       "2       508             SFG_R_2_1      SFG_R_291_2   \n",
       "1       509             SFG_L_1_1      SFG_L_290_2   \n",
       "\n",
       "                              Immediate.child.children  \n",
       "509  Telencephalon_L_501_5, Telencephalon_R_502_5, ...  \n",
       "508    Ventricle_498_4, Sulcus_L_499_4, Sulcus_R_500_4  \n",
       "507     Myelencephalon_L_494_4, Myelencephalon_R_495_4  \n",
       "506       Metencephalon_L_492_4, Metencephalon_R_493_4  \n",
       "505       Mesencephalon_L_490_4, Mesencephalon_R_491_4  \n",
       "..                                                 ...  \n",
       "5                                                  NaN  \n",
       "4                                                  NaN  \n",
       "3                                                  NaN  \n",
       "2                                                  NaN  \n",
       "1                                                  NaN  \n",
       "\n",
       "[509 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the multilevel lookup table\n",
    "\n",
    "def clean_multilevel(multilevel, adjacency_matrix):\n",
    "    \"\"\" Function that cleans the multilevel lookup table\n",
    "    \n",
    "    This function cleans the raw multilevel lookup table so that parents come before children. \n",
    "    The multilevel lookup table uses datasets from https://mricloud.org\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    multilevel: pandas.DataFrame\n",
    "        multilevel lookup table, whose columns are \"Structure,\" \"Immediate.parent,\" and \n",
    "        \"Immediate.child.children,\" and each row is a structure in the ontology. \n",
    "    \n",
    "    adjacency_matrix: pandas.DataFrame\n",
    "        A cleaned adjacency matrix (parents come before children)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pandas.DataFrame\n",
    "        Cleaned multilevel lookup table (parents come before children)\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    # Reverse the order of rows\n",
    "    multilevel = multilevel[::-1]\n",
    "    \n",
    "    # Reassign the numbers\n",
    "    multilevel.Number = range(1, adjacency_matrix.shape[0] + 1) \n",
    "    \n",
    "    return multilevel\n",
    "\n",
    "multilevel = clean_multilevel(multilevel, adjacency_matrix = adjacency_matrix)\n",
    "multilevel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b555c10",
   "metadata": {},
   "source": [
    "## 2. Create a subset of the adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e0002d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Everything</th>\n",
       "      <th>Telencephalon_L_501_5</th>\n",
       "      <th>CerebralCortex_L_482_4</th>\n",
       "      <th>Limbic_L_434_3</th>\n",
       "      <th>Hippo_L_338_2</th>\n",
       "      <th>Amyg_L_336_2</th>\n",
       "      <th>Hippo_L_75_1</th>\n",
       "      <th>Amyg_L_73_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Everything</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Telencephalon_L_501_5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CerebralCortex_L_482_4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Limbic_L_434_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hippo_L_338_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amyg_L_336_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hippo_L_75_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amyg_L_73_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Everything  Telencephalon_L_501_5  \\\n",
       "Everything                       0                      1   \n",
       "Telencephalon_L_501_5            0                      0   \n",
       "CerebralCortex_L_482_4           0                      0   \n",
       "Limbic_L_434_3                   0                      0   \n",
       "Hippo_L_338_2                    0                      0   \n",
       "Amyg_L_336_2                     0                      0   \n",
       "Hippo_L_75_1                     0                      0   \n",
       "Amyg_L_73_1                      0                      0   \n",
       "\n",
       "                        CerebralCortex_L_482_4  Limbic_L_434_3  Hippo_L_338_2  \\\n",
       "Everything                                   0               0              0   \n",
       "Telencephalon_L_501_5                        1               0              0   \n",
       "CerebralCortex_L_482_4                       0               1              0   \n",
       "Limbic_L_434_3                               0               0              1   \n",
       "Hippo_L_338_2                                0               0              0   \n",
       "Amyg_L_336_2                                 0               0              0   \n",
       "Hippo_L_75_1                                 0               0              0   \n",
       "Amyg_L_73_1                                  0               0              0   \n",
       "\n",
       "                        Amyg_L_336_2  Hippo_L_75_1  Amyg_L_73_1  \n",
       "Everything                         0             0            0  \n",
       "Telencephalon_L_501_5              0             0            0  \n",
       "CerebralCortex_L_482_4             0             0            0  \n",
       "Limbic_L_434_3                     1             0            0  \n",
       "Hippo_L_338_2                      0             1            0  \n",
       "Amyg_L_336_2                       0             0            1  \n",
       "Hippo_L_75_1                       0             0            0  \n",
       "Amyg_L_73_1                        0             0            0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subset_matrix_creator(subset_leaf_list, adjacency_matrix, multilevel):\n",
    "    ''' Function that creates a subset matrix\n",
    "    \n",
    "    This function creates a subset of the adjacency matrix using the user-specified structures.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    subset_leaf_list: list\n",
    "        List of leaf structures to include in the subset\n",
    "    \n",
    "    adjacency_matrix: pandas.DataFrame\n",
    "        The cleaned adjacency matrix\n",
    "    \n",
    "    multilevel: pandas.DataFrame\n",
    "        The cleaned multilevel lookup table\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pandas.DataFrame\n",
    "        A square subset matrix of the adjacency matrix that includes the user-specified leaf structures and \n",
    "        all of their parents\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    full_subset_list = subset_leaf_list # This list will get filled\n",
    "    iterations = 5 * len(subset_leaf_list) - 1\n",
    "    \n",
    "    for i in range(0, len(subset_leaf_list)): # For each leaf structure\n",
    "        for j in range(0, iterations): # Iterate over the 4 other levels in this ontology (5 levels * number of leaves)\n",
    "\n",
    "            # Structure index\n",
    "            structure_index = int(np.where(multilevel[\"Structure\"] == full_subset_list[j])[0])\n",
    "            structure_index = (adjacency_matrix.shape[0]) - structure_index\n",
    "            structure = multilevel[\"Structure\"][structure_index]\n",
    "\n",
    "            full_subset_list.append(multilevel[\"Immediate.parent\"][structure_index])\n",
    "        \n",
    "    full_subset_list = set(full_subset_list)\n",
    "    full_subset_list = pd.DataFrame(full_subset_list)\n",
    "        \n",
    "    pattern = r\"_[0-9]+_\"\n",
    "    structure_numbers = []\n",
    "    \n",
    "    for i in range(0, full_subset_list.shape[0]):\n",
    "        if full_subset_list[0][i] == \"Everything\":\n",
    "            structure_numbers.append(float(\"inf\")) # Assign the number \"inf\" to \"Everything\" for flexibility\n",
    "        else:\n",
    "            number = re.findall(pattern, full_subset_list[0][i])[0]\n",
    "            number = re.sub(\"[^0-9]\", \"\", number)\n",
    "            number = int(number)\n",
    "            structure_numbers.append(number)\n",
    "    \n",
    "    full_subset_list[\"structure_numbers\"] = structure_numbers\n",
    "    full_subset_list = full_subset_list.sort_values(by = [\"structure_numbers\"], axis = 0)\n",
    "    full_subset_list = full_subset_list[0].tolist()\n",
    "    \n",
    "    # Index the rows and columns of the adjacency matrix by these structures to create a subset\n",
    "    subset = adjacency_matrix[full_subset_list]\n",
    "    subset = subset.loc[full_subset_list]\n",
    "\n",
    "    # Reverse the order of rows and columns in the subset of the adjacency matrix\n",
    "    cols = subset.columns.tolist()\n",
    "    cols = cols[::-1]\n",
    "    subset = subset[cols]\n",
    "    subset = subset[::-1]\n",
    "    \n",
    "    return subset\n",
    "\n",
    "\n",
    "# User inputs a list of leaf structures\n",
    "subset_leaf_list = [\"Amyg_L_73_1\", \"Hippo_L_75_1\"]\n",
    "\n",
    "subset = subset_matrix_creator(subset_leaf_list, adjacency_matrix, multilevel)\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606ec92",
   "metadata": {},
   "source": [
    "## 3. Creating adjacency matrices of descendants and ancestors for the subset\n",
    "\n",
    "Note: the `adjacency_descendants()` function is designed only for a 5-level ontology since it contains a certain line of code that is repeated 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37323103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [False, False,  True,  True,  True,  True,  True,  True],\n",
       "       [False, False, False,  True,  True,  True,  True,  True],\n",
       "       [False, False, False, False,  True,  True,  True,  True],\n",
       "       [False, False, False, False, False, False,  True, False],\n",
       "       [False, False, False, False, False, False, False,  True],\n",
       "       [False, False, False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adjacency_descendants(adjacency_matrix, N, mu):\n",
    "    ''' Function that creates an adjacency matrix of descendants\n",
    "    \n",
    "    Sometimes we're interesting in querying whether one structure is a descendant of the other as opposed to \n",
    "    a direct child.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    adjacency_matrix: binary numpy.array\n",
    "        The cleaned adjacency matrix\n",
    "    \n",
    "    N: int\n",
    "        The number of samples\n",
    "    \n",
    "    mu: float\n",
    "        The difference in means (generally unknown)\n",
    "    \n",
    "    Note: this function may only be used for an ontology that has 6 levels (\"Everything\" is the \n",
    "    highest/most general level).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    binary numpy.array\n",
    "        Transitive adjacency matrix \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    M = adjacency_matrix.shape[0] # Total number of unique structures\n",
    "    names_full = adjacency_matrix.columns # List of the 509 structures' names\n",
    "    A = np.array(adjacency_matrix, dtype = bool)\n",
    "    Descendants = np.copy(A)\n",
    "    \n",
    "    Descendants = np.logical_or(Descendants,Descendants@A)\n",
    "    Descendants = np.logical_or(Descendants,Descendants@A)\n",
    "    Descendants = np.logical_or(Descendants,Descendants@A)\n",
    "    Descendants = np.logical_or(Descendants,Descendants@A)\n",
    "    Descendants = np.logical_or(Descendants,Descendants@A)\n",
    "    Descendants = np.logical_or(Descendants,Descendants@A)\n",
    "    \n",
    "    return Descendants\n",
    "\n",
    "descendants = adjacency_descendants(subset, N=20, mu=3.0)\n",
    "descendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62a74d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False, False, False],\n",
       "       [ True, False, False, False, False, False, False, False],\n",
       "       [ True,  True, False, False, False, False, False, False],\n",
       "       [ True,  True,  True, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True, False, False, False, False],\n",
       "       [ True,  True,  True,  True, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True, False,  True, False, False]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adjacency_ancestors(adjacency_matrix, N, mu):\n",
    "    ''' Function that creates an adjacency matrix of ancestors\n",
    "    \n",
    "    Sometimes we're interesting in querying whether one structure is an ancestor of the other as opposed to \n",
    "    a direct parent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix: pandas.DataFrame\n",
    "        The cleaned adjacency matrix\n",
    "    \n",
    "    N: int\n",
    "        The number of samples\n",
    "    \n",
    "    mu: float\n",
    "        The difference in means (generally unknown)\n",
    "    \n",
    "    Note: this function may only be used for an ontology that has 6 levels (\"Everything\" is the \n",
    "    highest/most general level).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    binary numpy.array\n",
    "        Transitive adjacency matrix \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    M = adjacency_matrix.shape[0]\n",
    "    Descendants = adjacency_descendants(adjacency_matrix, N, mu)\n",
    "    Ancestors = Descendants.T # Take transpose of descendants matrix to get ancestors    \n",
    "    Ancestors_and_self = np.logical_or(Ancestors,np.eye(M))\n",
    "    \n",
    "    return Ancestors\n",
    "\n",
    "ancestors = adjacency_ancestors(subset, N=20, mu=3.0)\n",
    "ancestors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247c0e1",
   "metadata": {},
   "source": [
    "## 4. Professor Tward's functions for parameter estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a320a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x,mu=0.0):\n",
    "    ''' Standard normal distribution CDF\n",
    "    \n",
    "    A Gaussian probability density function with unit variance\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: float or numpy.array\n",
    "        A point to evaluate the function at, can be a numpy array\n",
    "    \n",
    "    mu: float\n",
    "        Mean of the gaussian (default 0)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    numpy.array\n",
    "        The Gaussian probability density function evaluated at specified point\n",
    "    \n",
    "    Note: this function is not called\n",
    "    \n",
    "    '''\n",
    "    return 1.0/np.sqrt(2.0*np.pi)*np.exp(-(x - mu)**2/2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c68efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_from_Q(Q,Ancestors_and_self):\n",
    "    ''' Function that calculates P from Q \n",
    "    \n",
    "    This function computes the marginal probability that a structure is affected given the conditional \n",
    "    probabilities that structures are affected conidtioned on their parents.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q: numpy.array\n",
    "        List of conditional probabilities \n",
    "        \n",
    "    Ancestors_and_self: transitive adjacency matrix \n",
    "        Adjacency matrix with loops (when a node is connected to itself)\n",
    "    \n",
    "    Returns \n",
    "    ----------\n",
    "    numpy.array\n",
    "        marginal probabilities \n",
    "    \n",
    "    Note: this function is not called\n",
    "    \n",
    "    '''\n",
    "    P = np.empty_like(Q)\n",
    "    for i in range(M):\n",
    "        P[i] = np.prod(Q[Ancestors_and_self[i,:]])\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "200ecf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_from_P(P,A):\n",
    "    ''' Function that calculates Q from P\n",
    "    \n",
    "    Given a list of marginal probabilities of structures being affected, compute the conditional probabilities \n",
    "    of a structure being affected given its parents.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    P: numpy.array\n",
    "        The marignal probabilities\n",
    "    \n",
    "    A: binary numpy.array\n",
    "        Adjacency matrix that describes parent to child relationships\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    numpy.array\n",
    "        The conditional probabilities\n",
    "    \n",
    "    '''\n",
    "    M = A.shape[0]\n",
    "    # now we need to calculate Q\n",
    "    Q = np.zeros_like(P)\n",
    "    Q[0] = P[0]\n",
    "    for i in range(1,M):\n",
    "        Q[i] = P[i] / P[A[:,i]]\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba23da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_P(X, mu, A, Descendants_and_self, draw=False, niter=100, P0=None, names=None, clip=0.001):\n",
    "    ''' Function for estimating P\n",
    "    \n",
    "    Apply an EM algorithm to estimate marginal probabiltiies that each structure is affected given a dataset.\n",
    "    We assume data is normally distributed with unit variance and mean mu.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy.array\n",
    "        Contains observations for each structure and each subject\n",
    "    \n",
    "    mu: float\n",
    "        The known mean for affected structures (unaffected structures have mean 0)\n",
    "    \n",
    "    A: binary numpy.array\n",
    "        Adjacency matrix describing parent child relationships\n",
    "    \n",
    "    Descendants_and_self: transitive adjacency matrix\n",
    "        Adjacency matrix of descendants. Can be computed from A, but here we use it as an input\n",
    "    \n",
    "    draw: int\n",
    "        Illustrates the data every `draw` iterations of EM algorithm. \n",
    "        draw = 0 or false means do not draw. Default value is False.\n",
    "    \n",
    "    iter: int\n",
    "        The number of iterations of em algorithm\n",
    "    \n",
    "    p0: float\n",
    "        The initial guess for marginal probabilities\n",
    "\n",
    "    names: list \n",
    "        The names (strings) of structures in ontology\n",
    "        \n",
    "    clip: float\n",
    "        Number that clips probabilities away from 0 or 1\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    numpy.array\n",
    "        Contains the marginal probabilities that structures are affected)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if draw: \n",
    "        f,ax = plt.subplots(2,2)\n",
    "        if names is None:\n",
    "            names = np.arange(A.shape[0])\n",
    "\n",
    "    N = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "    M = A.shape[0]\n",
    "    is_leaf = np.sum(Descendants_and_self, 1) == 1\n",
    "    \n",
    "    # okay now comes my algorithm\n",
    "    # initialize\n",
    "    if P0 is None:\n",
    "        P = np.ones(M)*0.5\n",
    "    else:\n",
    "        P = np.asarray(P0)\n",
    "    \n",
    "    for it in range(niter):\n",
    "        # calculate leaf posterior (this is prob of no effect)\n",
    "        #leaf_posterior = ((1.0-P[is_leaf])*phi(X))\n",
    "        #leaf_posterior = leaf_posterior/(leaf_posterior + P[is_leaf]*phi(X,mu) )\n",
    "        P_ = np.maximum(P, clip) # Clip probability: if P is very small, then set it to 0.001\n",
    "        P_ = np.minimum(P_, 1-clip) # Clip probability: if P_ is very big, then set it to 0.999\n",
    "        P_over_one_minus_P = P_/(1.0-P_)\n",
    "        #leaf_log_posterior = -np.log(1.0 + P_over_one_minus_P[is_leaf]*phi(X,mu)/phi(X) )\n",
    "        leaf_log_posterior = -np.log1p( P_over_one_minus_P[is_leaf]*phi(X,mu)/phi(X) )\n",
    "        \n",
    "\n",
    "        # calculate posterior for all structures\n",
    "        # now for each structure, I need a leaf likelihod, and an adjustment\n",
    "        #posterior = np.zeros((N,M))\n",
    "        log_posterior = np.zeros((N,M))\n",
    "        for i in range(M):\n",
    "            #posterior[:,i] = np.prod(leaf_posterior[:,Descendants_and_self[i,:][is_leaf]],1)\n",
    "            log_posterior[:,i] = np.sum(leaf_log_posterior[:,Descendants_and_self[i,:][is_leaf]],1)\n",
    "        \n",
    "        # calculate adjustment factor for correlations\n",
    "        Q = Q_from_P(P,A)\n",
    "        #adjustment_single = np.ones(M)\n",
    "        log_adjustment_single = np.zeros(M)\n",
    "        for i in range(M):\n",
    "            if is_leaf[i]:\n",
    "                continue\n",
    "            #adjustment_single[i] = (1.0 - P[i])/ ((1.0 - P[i]) + P[i]*np.prod(1.0 - Q[A[i,:]]))\n",
    "            #log_adjustment_single[i] = -np.log(1.0 + P_over_one_minus_P[i]*np.prod(1.0 - Q[A[i,:]]))\n",
    "            log_adjustment_single[i] = -np.log1p(P_over_one_minus_P[i]*np.prod(1.0 - Q[A[i,:]]))\n",
    "            \n",
    "        \n",
    "        # now my adjust ment requres products of all descendants\n",
    "        #adjustment = np.ones(M)\n",
    "        log_adjustment = np.ones(M)\n",
    "        for i in range(M):\n",
    "            #adjustment[i] = np.prod(adjustment_single[Descendants_and_self[i,:]])\n",
    "            log_adjustment[i] = np.sum(log_adjustment_single[Descendants_and_self[i,:]])\n",
    "            \n",
    "\n",
    "        # calculate the adjusted posterior\n",
    "        #posterior = posterior*adjustment\n",
    "        log_posterior = log_posterior + log_adjustment\n",
    "        \n",
    "        #P = np.sum(1.0 - posterior,0)/N        \n",
    "        #P = np.sum(1.0 - np.exp(log_posterior),0)/N\n",
    "        P = -np.sum(np.expm1(log_posterior),0)/N\n",
    "        posterior = np.exp(log_posterior)\n",
    "        \n",
    "        # draw        \n",
    "        if draw>0 and ( (not it%draw) or (it==niter-1)):     \n",
    "            \n",
    "            ax[0,0].cla()\n",
    "            ax[0,0].imshow(posterior, vmin = 0, vmax = 1)\n",
    "            ax[0,0].set_aspect('auto')\n",
    "            ax[0,0].set_title('P[Z=0|X] (prob not affected)')\n",
    "            ax[0,0].set_xticks(np.arange(M))\n",
    "            ax[0,0].set_xticklabels(names,rotation=15, fontsize = 5)\n",
    "            ax[0,0].set_ylabel('Sample')\n",
    "\n",
    "            ax[0,1].cla()\n",
    "            ax[0,1].bar(np.arange(M),P)\n",
    "            ax[0,1].set_xticks(np.arange(M))\n",
    "            ax[0,1].set_xticklabels(names,rotation=15, fontsize = 5)\n",
    "            ax[0,1].set_ylim((0, 1))\n",
    "\n",
    "            f.canvas.draw()\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcdbd08",
   "metadata": {},
   "source": [
    "## 5. Generate samples, parameter estimation, permutation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fc5cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulated_data(filename, subset, case, n_repeats, N, mu):\n",
    "    ''' Function that generates simulated data\n",
    "    \n",
    "    This function generates the simulated data to be used for permutation testing\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: string\n",
    "        The user-specified filename to save the generated data; make sure to give it a name that's different \n",
    "        from the file name you'll specify for permutation testing results so they're separate files and won't \n",
    "        be overwritten.\n",
    "    \n",
    "    subset: pandas.DataFrame\n",
    "        The subset adjacency matrix\n",
    "    \n",
    "    case: int\n",
    "        The case number (1 = nothing is affected, 2 = left hippocampus is affected, 3 = both left hippocampus \n",
    "        and left amygdala are affected, 4 = either left hippocampus or left amygdala is affected but not both)\n",
    "    \n",
    "    n_repeats: int\n",
    "        The number of repeats. We want to generate a random dataset with the same parameters but `n_repeats` \n",
    "        different realizations of the random variables.\n",
    "    \n",
    "    N: int\n",
    "        The number of samples\n",
    "    \n",
    "    mu: float\n",
    "        The difference in means (assume it's known)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    npz file (written to disk, not explicitly returned)\n",
    "        The 1st array is X (probability of being affected for each sample), the 2nd array is Z (which \n",
    "        structures are affected or unaffected for each sample), and the 3rd array is G (whether each sample is \n",
    "        actually affected)\n",
    "  \n",
    "    '''\n",
    "    \n",
    "    M = subset.shape[0] # Number of total unique structures\n",
    "    \n",
    "    for j in range(n_repeats):\n",
    "        outputs = [] # Empty list for each iteration\n",
    "        Z = np.zeros((N,M)) # Initialize Z, which will be a binary variable that tells us if a structure is affected\n",
    "        Naffected = N // 2 # Don't set Naffected to 0 or else there won't be any samples\n",
    "        number_of_leaves = np.count_nonzero(np.sum(subset, 1) == 0) # Number of leaf structures (zero children)\n",
    "        \n",
    "        if case == 1:\n",
    "            pass\n",
    "        elif case == 2:\n",
    "            for i in range(N):\n",
    "                if i < Naffected: # Assume that the first half of samples are affected and second half are not\n",
    "                    Z[i][6] = 1 # Left hippocampus is affected\n",
    "        elif case == 3:\n",
    "            for i in range(N):\n",
    "                if i < Naffected: # Assume that the first half of samples are affected and second half are not\n",
    "                    Z[i][6] = 1 # Left hippocampus is affected\n",
    "                    Z[i][7] = 1 # Left amygdala is affected\n",
    "        elif case == 4:\n",
    "            for i in range(N):\n",
    "                if i < Naffected: # Assume that the first half of samples are affected and second half are not\n",
    "                    if np.random.rand() < 0.5:\n",
    "                        Z[i][6] = 1 # Left hippocampus is affected\n",
    "                    else:\n",
    "                        Z[i][7] = 1 # Left amygdala is affected\n",
    "        \n",
    "            \n",
    "        is_leaf = np.concatenate([np.ones(number_of_leaves), np.zeros(M - number_of_leaves)]) # 1 for leaf structures, 0 for non-leaf structures\n",
    "        is_leaf = np.array(is_leaf, dtype = bool) # Convert is_leaf to the boolean type\n",
    "        is_leaf = is_leaf[::-1] # Data specific\n",
    "        m = np.sum(is_leaf) # Number of leaf structures (m = 2)\n",
    "            \n",
    "        G = np.arange(N) < Naffected # All falses since all samples are unaffected\n",
    "        X = Z[:, is_leaf > 0] * mu + np.random.randn(N, m)\n",
    "        \n",
    "        np.savez(filename, X = X, Z = Z, G = G)\n",
    "    \n",
    "generate_simulated_data(filename=\"test1_data\", subset=subset, case=2, n_repeats=10, N=20, mu=3.0)\n",
    "generate_simulated_data(filename=\"test2_data\", subset=subset, case=3, n_repeats=5, N=20, mu=3.0)\n",
    "generate_simulated_data(filename=\"test3_data\", subset=subset, case=4, n_repeats=15, N=20, mu=3.0)\n",
    "generate_simulated_data(filename=\"test4_data\", subset=subset, case=1, n_repeats=20, N=20, mu=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d1ba279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mX\u001b[0m\n",
      "[[ 2.22263656  1.59277346]\n",
      " [ 2.92512486  0.04355884]\n",
      " [ 1.56416404  2.24843694]\n",
      " [ 2.25993271 -1.46066158]\n",
      " [ 3.37759003  0.20918227]\n",
      " [ 2.55910665 -1.85560459]\n",
      " [ 3.10110528  0.98956362]\n",
      " [ 3.5557505   1.78729342]\n",
      " [ 2.53949835 -0.95093059]\n",
      " [ 2.59902296 -1.93783049]\n",
      " [ 0.25947211  0.90084115]\n",
      " [ 0.1095843  -1.10301842]\n",
      " [-0.78803713  0.77295319]\n",
      " [-0.90020676  1.58535819]\n",
      " [-0.2896158  -0.61192231]\n",
      " [ 0.16728972 -1.92096578]\n",
      " [ 0.09902726 -0.30614956]\n",
      " [-0.5579848  -0.14458222]\n",
      " [-0.18182667  1.71341805]\n",
      " [-0.82648249 -0.23816186]]\n",
      "\u001b[1mZ\u001b[0m\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\u001b[1mG\u001b[0m\n",
      "[ True  True  True  True  True  True  True  True  True  True False False\n",
      " False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# Viewing the contents of the generated data file to check\n",
    "test = np.load(\"test1_data.npz\")\n",
    "\n",
    "print(\"\\033[1m\" + \"X\" + \"\\033[0m\") \n",
    "print(test[\"X\"])\n",
    "\n",
    "print(\"\\033[1m\" + \"Z\" + \"\\033[0m\") \n",
    "print(test[\"Z\"])\n",
    "\n",
    "print(\"\\033[1m\" + \"G\" + \"\\033[0m\") \n",
    "print(test[\"G\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36a8019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sorting function that sorts using the dictionary\n",
    "def sorting_function(input_string, dictionary):\n",
    "    ''' Function that sorts results by posterior probability\n",
    "    \n",
    "    This function sorts a list of strings by posterior probability, where parents come before children when \n",
    "    there are ties.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_string: list\n",
    "        A list of strings to be sorted; each string contains structure name, posterior probability, p-value\n",
    "    \n",
    "    dictionary: dictionary\n",
    "        A dictionary whose keys are the structures in the subset and values are 1 through the number of \n",
    "        structures in the subset (must be in order from parents to children)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        A sorted list of strings (sorted by posterior probability, where parents come before children when \n",
    "        there are ties)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    my_list = input_string\n",
    "    my_list = sorted(my_list, key = lambda x : dictionary[x.split(\",\")[0]])  \n",
    "    my_list = sorted(my_list, key = lambda x : float(x.split(\",\")[1].split(\"=\")[-1]), reverse = True) \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c8af947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_testing(filename_old, filename_new, subset, n_repeats, nperm, N, mu, niter, clip, initial_prob):\n",
    "    ''' Function that conducts permutation testing\n",
    "    \n",
    "    This function conducts permutation testing using the generated data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename_old: string\n",
    "        The filename of the generated data\n",
    "    \n",
    "    filename_new: string\n",
    "        The user-specified filename for the permutation testing results (choose a different name from \n",
    "        filename_old if you don't want generated data to get overwritten by permutation testing results)\n",
    "    \n",
    "    subset: pandas.DataFrame\n",
    "        The subset adjacency matrix\n",
    "    \n",
    "    n_repeats: int\n",
    "        The number of repeats. We generated a random dataset with the same parameters but `n_repeats` \n",
    "        different realizations of the random variables. `n_repeats` must be the same value as `n_repeats`\n",
    "        when we generated data earlier.\n",
    "    \n",
    "    nperm: int\n",
    "        The number of permutations for permutation testing \n",
    "    \n",
    "    N: int\n",
    "        The number of samples. N must be the same value as N from generating data earlier.\n",
    "    \n",
    "    mu: float\n",
    "        The difference in means (generally unknown). mu should be the same value as mu from generating data\n",
    "        earlier in order to get meaningful results. But, mu doesn't have to be the same if you don't want to\n",
    "        make it the same.\n",
    "    \n",
    "    niter: int\n",
    "        The number of iterations of the EM algorithm\n",
    "    \n",
    "    clip: float\n",
    "        Number that clips probabilities away from 0 or 1\n",
    "    \n",
    "    initial_prob: float\n",
    "        The intial probability\n",
    "    \n",
    "    Returns \n",
    "    ----------\n",
    "    npz file (written to disk, not explicitly returned)\n",
    "        The 1st array contains p-values, 2nd array contains the names of the structures in the subset, 3rd \n",
    "        array contains the posterior probabilities, and 4th array contains the information from the prior 3 \n",
    "        arrays saved in 1 string per structure.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    M = subset.shape[0] # Number of total unique structures\n",
    "    S = np.array(subset, dtype = bool)\n",
    "    names_subset = subset.columns # List of the 8 structures' names\n",
    "    Descendants = adjacency_descendants(subset, N=N, mu=mu)\n",
    "    Descendants_and_self = np.logical_or(Descendants, np.eye(M))\n",
    "    \n",
    "    # Load the generated data\n",
    "    data = np.load(filename_old)\n",
    "    X = data[\"X\"]\n",
    "    Z = data[\"Z\"]\n",
    "    G = data[\"G\"]\n",
    "    \n",
    "    for j in range(n_repeats):\n",
    "        outputs = [] # Empty list for each iteration\n",
    "        \n",
    "        ### PARAMETER ESTIMATION ###\n",
    "    \n",
    "        P_subset = np.ones(M) * 0.5 # Array of 8 copies of 0.5\n",
    "        Q = Q_from_P(P_subset, S)\n",
    "\n",
    "        P0 = np.ones(M) * initial_prob\n",
    "        P_subset = estimate_P(X[G], mu, S, Descendants_and_self, draw=0, P0=P0, niter=niter, names=names_subset, clip=clip)\n",
    "        # Set draw = 0 to prevent drawing the graphs\n",
    "        \n",
    "        ### GENERATING PERMUTED DATA ###\n",
    "    \n",
    "        Ps = []\n",
    "        for n in range(nperm):\n",
    "            Xp = X[np.random.permutation(N)[G]]\n",
    "            P_ = estimate_P(Xp,mu,S,Descendants_and_self,draw=0,niter=niter,P0=P0)\n",
    "            Ps.append(P_)\n",
    "\n",
    "        Ps_sort = np.array([np.sort(Pi)[::-1] for Pi in Ps])\n",
    "        \n",
    "        ### PERMUTATION TESTING ###\n",
    "    \n",
    "        inds = np.argsort(P_subset)[::-1]\n",
    "        pval = np.zeros_like(P_subset)\n",
    "        alpha = 0.05\n",
    "        \n",
    "        pval_list = [] # Empty list to be filled\n",
    "        names_list = [] # Empty list to be filled\n",
    "        posterior_list = [] # Empty list to be filled\n",
    "        \n",
    "        for i in range(M):    \n",
    "            pval[inds[i]] = np.mean(Ps_sort[:,i] >= P_subset[inds[i]])\n",
    "            outputs.append(f\"{names_subset[inds[i]]}, P[Z=1|X]={P_subset[inds[i]]}, p={pval[inds[i]]}\")\n",
    "            # Every structure that gets rejected gets an entry\n",
    "            \n",
    "            pval_list.append(pval[inds[i]])\n",
    "            names_list.append(names_subset[inds[i]])\n",
    "            posterior_list.append(P_subset[inds[i]])\n",
    "        \n",
    "        ### SORT THE POSTERIOR VALUES ###\n",
    "        # Use the subset adjacency matrix to create a dictionary\n",
    "        columns = np.array(subset.columns)\n",
    "        dictionary = dict(enumerate(columns.flatten(), 1))\n",
    "        dictionary = dict((value, key) for key, value in dictionary.items()) # Swap the keys and values\n",
    "        outputs = sorting_function(outputs, dictionary)\n",
    "        \n",
    "        ### SAVE DATA ### \n",
    "        \n",
    "        np.savez(filename_new, pval = pval_list, names = names_list, posterior = posterior_list, strings = outputs)\n",
    "    \n",
    "\n",
    "    \n",
    "permutation_testing(filename_old=\"test1_data.npz\", filename_new = \"test1_results\", subset=subset, n_repeats=10, nperm=10, N=20, mu=3.0, niter=5, initial_prob = 0.5, clip=0.001)\n",
    "permutation_testing(filename_old=\"test2_data.npz\", filename_new = \"test2_results\", subset=subset, n_repeats=10, nperm=10, N=20, mu=3.0, niter=5, clip=0.0001, initial_prob = 0.5)\n",
    "permutation_testing(filename_old=\"test3_data.npz\", filename_new = \"test3_results\", subset=subset, n_repeats=10, nperm=10, N=20, mu=3.0, niter=5, initial_prob = 0.25, clip=0.01)\n",
    "permutation_testing(filename_old=\"test4_data.npz\", filename_new = \"test4_results\", subset=subset, n_repeats=10, nperm=10, N=20, mu=3.0, niter=5, initial_prob = 0.75, clip=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57911783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtest1_results.npz\u001b[0m\n",
      "\u001b[1mp-values:\u001b[0m\n",
      "[0.  0.  0.  0.  0.  0.  0.3 0.3]\n",
      "\u001b[1mnames:\u001b[0m\n",
      "['Limbic_L_434_3' 'CerebralCortex_L_482_4' 'Telencephalon_L_501_5'\n",
      " 'Everything' 'Hippo_L_75_1' 'Hippo_L_338_2' 'Amyg_L_73_1' 'Amyg_L_336_2']\n",
      "\u001b[1mposterior:\u001b[0m\n",
      "[0.99992995 0.99992995 0.99992995 0.99992995 0.99988072 0.99988072\n",
      " 0.08613401 0.08613401]\n",
      "\u001b[1mstrings:\u001b[0m\n",
      "['Everything, P[Z=1|X]=0.9999299482092591, p=0.0'\n",
      " 'Telencephalon_L_501_5, P[Z=1|X]=0.9999299482092591, p=0.0'\n",
      " 'CerebralCortex_L_482_4, P[Z=1|X]=0.9999299482092591, p=0.0'\n",
      " 'Limbic_L_434_3, P[Z=1|X]=0.9999299482092591, p=0.0'\n",
      " 'Hippo_L_338_2, P[Z=1|X]=0.9998807197146906, p=0.0'\n",
      " 'Hippo_L_75_1, P[Z=1|X]=0.9998807197146906, p=0.0'\n",
      " 'Amyg_L_336_2, P[Z=1|X]=0.08613400950566785, p=0.3'\n",
      " 'Amyg_L_73_1, P[Z=1|X]=0.08613400950566785, p=0.3']\n",
      "\u001b[1mtest2_results.npz\u001b[0m\n",
      "\u001b[1mp-values:\u001b[0m\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\u001b[1mnames:\u001b[0m\n",
      "['Limbic_L_434_3' 'CerebralCortex_L_482_4' 'Telencephalon_L_501_5'\n",
      " 'Everything' 'Amyg_L_73_1' 'Amyg_L_336_2' 'Hippo_L_75_1' 'Hippo_L_338_2']\n",
      "\u001b[1mposterior:\u001b[0m\n",
      "[1.         1.         1.         1.         0.99999751 0.99999751\n",
      " 0.99999085 0.99999085]\n",
      "\u001b[1mstrings:\u001b[0m\n",
      "['Everything, P[Z=1|X]=0.9999999999982127, p=0.0'\n",
      " 'Telencephalon_L_501_5, P[Z=1|X]=0.9999999999982127, p=0.0'\n",
      " 'CerebralCortex_L_482_4, P[Z=1|X]=0.9999999999982127, p=0.0'\n",
      " 'Limbic_L_434_3, P[Z=1|X]=0.9999999999982127, p=0.0'\n",
      " 'Amyg_L_336_2, P[Z=1|X]=0.9999975093001826, p=0.0'\n",
      " 'Amyg_L_73_1, P[Z=1|X]=0.9999975093001826, p=0.0'\n",
      " 'Hippo_L_338_2, P[Z=1|X]=0.9999908520910488, p=0.0'\n",
      " 'Hippo_L_75_1, P[Z=1|X]=0.9999908520910488, p=0.0']\n",
      "\u001b[1mtest3_results.npz\u001b[0m\n",
      "\u001b[1mp-values:\u001b[0m\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\u001b[1mnames:\u001b[0m\n",
      "['Limbic_L_434_3' 'CerebralCortex_L_482_4' 'Telencephalon_L_501_5'\n",
      " 'Everything' 'Amyg_L_73_1' 'Amyg_L_336_2' 'Hippo_L_75_1' 'Hippo_L_338_2']\n",
      "\u001b[1mposterior:\u001b[0m\n",
      "[0.98879546 0.98879546 0.98879546 0.98879546 0.68189922 0.68189922\n",
      " 0.24396841 0.24396841]\n",
      "\u001b[1mstrings:\u001b[0m\n",
      "['Everything, P[Z=1|X]=0.988795455140627, p=0.0'\n",
      " 'Telencephalon_L_501_5, P[Z=1|X]=0.988795455140627, p=0.0'\n",
      " 'CerebralCortex_L_482_4, P[Z=1|X]=0.988795455140627, p=0.0'\n",
      " 'Limbic_L_434_3, P[Z=1|X]=0.988795455140627, p=0.0'\n",
      " 'Amyg_L_336_2, P[Z=1|X]=0.6818992168006806, p=0.0'\n",
      " 'Amyg_L_73_1, P[Z=1|X]=0.6818992168006806, p=0.0'\n",
      " 'Hippo_L_338_2, P[Z=1|X]=0.24396840778747575, p=0.0'\n",
      " 'Hippo_L_75_1, P[Z=1|X]=0.24396840778747575, p=0.0']\n",
      "\u001b[1mtest4_results.npz\u001b[0m\n",
      "\u001b[1mp-values:\u001b[0m\n",
      "[0.8 0.8 0.8 0.8 1.  1.  0.5 0.5]\n",
      "\u001b[1mnames:\u001b[0m\n",
      "['Limbic_L_434_3' 'CerebralCortex_L_482_4' 'Telencephalon_L_501_5'\n",
      " 'Everything' 'Hippo_L_75_1' 'Hippo_L_338_2' 'Amyg_L_73_1' 'Amyg_L_336_2']\n",
      "\u001b[1mposterior:\u001b[0m\n",
      "[7.31541505e-02 7.31541505e-02 7.31541505e-02 7.31541505e-02\n",
      " 8.70191095e-05 8.70191095e-05 7.91281011e-05 7.91281011e-05]\n",
      "\u001b[1mstrings:\u001b[0m\n",
      "['Everything, P[Z=1|X]=0.07315415050879942, p=0.8'\n",
      " 'Telencephalon_L_501_5, P[Z=1|X]=0.07315415050879942, p=0.8'\n",
      " 'CerebralCortex_L_482_4, P[Z=1|X]=0.07315415050879942, p=0.8'\n",
      " 'Limbic_L_434_3, P[Z=1|X]=0.07315415050879942, p=0.8'\n",
      " 'Hippo_L_338_2, P[Z=1|X]=8.701910949511537e-05, p=1.0'\n",
      " 'Hippo_L_75_1, P[Z=1|X]=8.701910949511537e-05, p=1.0'\n",
      " 'Amyg_L_336_2, P[Z=1|X]=7.912810109383051e-05, p=0.5'\n",
      " 'Amyg_L_73_1, P[Z=1|X]=7.912810109383051e-05, p=0.5']\n"
     ]
    }
   ],
   "source": [
    "# Viewing the contents of the .npz files to check\n",
    "# Using \"\\033[1m\" and \"\\033[0m\" to get bold text printed\n",
    "\n",
    "filename = [\"test1_results.npz\", \"test2_results.npz\", \"test3_results.npz\", \"test4_results.npz\"]\n",
    "for i in range(0, 4):\n",
    "    # Assign the file to an object called \"test,\" which is a dictionary object\n",
    "    test = np.load(filename[i])\n",
    "    print(\"\\033[1m\" + filename[i] + \"\\033[0m\") \n",
    "\n",
    "    # Print the values corresponding to each key\n",
    "    print(\"\\033[1m\" + \"p-values:\" + \"\\033[0m\")\n",
    "    print(test[\"pval\"])\n",
    "    \n",
    "    print(\"\\033[1m\" + \"names:\" + \"\\033[0m\")\n",
    "    print(test[\"names\"])\n",
    "    \n",
    "    print(\"\\033[1m\" + \"posterior:\" + \"\\033[0m\")\n",
    "    print(test[\"posterior\"])\n",
    "    \n",
    "    print(\"\\033[1m\" + \"strings:\" + \"\\033[0m\")\n",
    "    print(test[\"strings\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392ee3d",
   "metadata": {},
   "source": [
    "## 6. Calculating false positive rate\n",
    "\n",
    "**Note 1: We can only calculate the false positive rate for cases 1 and 2, not 3 or 4. The .npz files that are used in the demonstration below contain case 3 and 4 data in addition to case 1 and 2 data, so this would not be a correct calculation.**  \n",
    "\n",
    "**Note 2: The input `n_repeats` should be the singular number of repeats that were used to generate every .npz file. In this example, each .npz file was generated using different numbers of repeats, and then they're used to calculate the same false positive rate. This is not correct usage, it is just for example purposes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e7e9469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def false_positive_rate(subset, file_names, n_repeats):\n",
    "    ''' False positive rate function\n",
    "    \n",
    "    This function calculates the false positive rate after permutation testing\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subset: pandas.DataFrame\n",
    "        The subset adjacency matrix\n",
    "    \n",
    "    file_names: list\n",
    "        A list containing the file names of the permutation testing results to be used in the calculation\n",
    "    \n",
    "    n_repeats: int\n",
    "        The number of repeats. We generated a random dataset with the same parameters but `n_repeats` \n",
    "        different realizations of the random variables. `n_repeats` must be the same value for each file in the \n",
    "        file_name list.\n",
    "    \n",
    "    Note: we can only calculate false positive rates for cases 1 and 2; make sure the input files in the \n",
    "    file_name list were all created for the same case (either case 1 or case 2).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    float\n",
    "        The false positive rate from permutation testing\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Use the subset adjacency matrix to create a dictionary\n",
    "    columns = np.array(subset.columns)\n",
    "    dictionary = dict(enumerate(columns.flatten(), 1))\n",
    "    dictionary = dict((value, key) for key, value in dictionary.items()) # Swap the keys and values\n",
    "    \n",
    "    # False positive rate calculation\n",
    "    count = 0\n",
    "    for i in range(0, len(file_names)):\n",
    "        reject_p = np.zeros(len(columns)) # For each repeat, assume no structures are affected (null hypothesis)\n",
    "        file = np.load(file_names[i])\n",
    "        file = file[\"strings\"] # Values corresponding to the key\n",
    "        file = sorting_function(file, dictionary)\n",
    "        \n",
    "        for j in range(0, len(columns)): # For each structure in each repeat...\n",
    "            p = file[j].find(\"p=\") # Index of \"p\" in the string\n",
    "            pval = float(file[j][p::][2::]) # Extract the p-value\n",
    "            if pval < 0.05: # If the p-value is < 0.05...\n",
    "                reject_p[j] = 1\n",
    "            else: # The test statistic is the probabilities, which were sorted, so stop after the first structure we fail to reject\n",
    "                break\n",
    "                \n",
    "        if any(reject_p > 0): # If there's at least one structure with p < 0.05...\n",
    "            count += 1 # Add 1 to the false positive count\n",
    "                \n",
    "    return (count / n_repeats)\n",
    "    \n",
    "\n",
    "false_positive_rate(subset, file_names = [\"test1_results.npz\", \"test2_results.npz\", \"test3_results.npz\", \"test4_results.npz\"], n_repeats = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00990428",
   "metadata": {},
   "source": [
    "## 7. Calculating false negative rate\n",
    "\n",
    "**Note 1: We can only calculate the false negative rate for cases 2, 3, and 4, not case 1. The .npz files that are used in the demonstration below contain case 1 data in addition to case 2, 3, and 4 data, so this would not be a correct calculation.**\n",
    "\n",
    "**Note 2: The input `n_repeats` should be the singular number of repeats that were used to generate every .npz file. In this example, each .npz file was generated using different numbers of repeats, and then they're used to calculate the same false negative rate. This is not correct usage, it is just for example purposes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "155d8e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negative_rate(subset, file_names, case, n_repeats):\n",
    "    ''' False negative rate function\n",
    "    \n",
    "    This function calculates the false negative rate after permutation testing\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subset: pandas.DataFrame\n",
    "        The subset adjacency matrix\n",
    "    \n",
    "    file_names: list\n",
    "        A list containing the file names of the permutation testing results to be used in the calculation\n",
    "    \n",
    "    case: int\n",
    "        The case that was used to generate the data; all files in the file_names list must correspond to the \n",
    "        same case.\n",
    "    \n",
    "    n_repeats: int\n",
    "        The number of repeats. We generated a random dataset with the same parameters but `n_repeats` \n",
    "        different realizations of the random variables. `n_repeats` must be the same value for each file in the \n",
    "        file_name list.\n",
    "        \n",
    "    Note: we can only calculate false negative rates for cases 2, 3, and 4; make sure the input files in the \n",
    "    file_name list were all created for the same case (either case 2 or case 3 or case 4).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pandas.DataFrame\n",
    "        Data frame whose columns are \"Structure\" and \"False negative rate\" with one row per structure\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Use the subset adjacency matrix to create a dictionary\n",
    "    columns = np.array(subset.columns)\n",
    "    dictionary = dict(enumerate(columns.flatten(), 1))\n",
    "    dictionary = dict((value, key) for key, value in dictionary.items()) # Swap the keys and values\n",
    "    \n",
    "    # Create a counter for each structure\n",
    "    counters = [] # Empty list to be filled\n",
    "    for k in range(0, len(columns)): # For each structure in each repeat...\n",
    "        counters.append(0) # List of counters, one entry for each structure, each counter starts at 0\n",
    "    \n",
    "    # False negative rate calculation\n",
    "    for i in range(0, len(file_names)):\n",
    "        reject_p = np.zeros(len(columns)) # For each repeat, assume no structures are affected (null hypothesis)\n",
    "        file = np.load(file_names[i])\n",
    "        file = file[\"strings\"] # Values corresponding to the key\n",
    "        file = sorting_function(file, dictionary)\n",
    "    \n",
    "        for j in range(0, len(columns)): # For each structure in each repeat...\n",
    "            p = file[j].find(\"p=\") # Index of \"p\" in the string\n",
    "            pval = float(file[j][p::][2::]) # Extract the p-value\n",
    "            if pval < 0.05: # If the p-value is < 0.05...\n",
    "                reject_p[j] = 1\n",
    "            else: # The test statistic is the probabilities, which were sorted, so stop after the first structure we fail to reject\n",
    "                break\n",
    "        \n",
    "        # Calculate false negative rate the same for cases 3 or 4\n",
    "        if ((case == 3) or (case == 4)): # Go through every structure since both hippocampus and amygdala are affected\n",
    "            for k in range(0, len(columns)): # For each structure in each repeat...\n",
    "                for r, s in zip(reject_p, file):\n",
    "                    if columns[k] in s and r: # If a given structure name is in the string and we decided to reject H0...\n",
    "                        counters[k] += 1\n",
    "        \n",
    "        # Calculate false negative rate differently for case 2\n",
    "        elif case == 2:\n",
    "            for k in range(0, len(columns)): # For each structure in each repeat...\n",
    "                for r, s in zip(reject_p, file):\n",
    "                    if \"Amyg\" in s and r:\n",
    "                        continue # Don't consider the amygdala structures for case 2\n",
    "                    if columns[k] in s and r: # If a given structure name is in the string and we decided to reject H0...\n",
    "                        counters[k] += 1\n",
    "        \n",
    "    output = pd.DataFrame({\"Structure\": columns, \"False negative rate\": 1 - np.array(counters) / n_repeats})\n",
    "    \n",
    "    if case == 2:\n",
    "        output = output[~output[\"Structure\"].str.startswith(\"Amyg\")] # Omit the rows corresponding to amygdala structures\n",
    "    \n",
    "    return output                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4484592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTesting case 2:\u001b[0m\n",
      "                Structure  False negative rate\n",
      "0              Everything                  0.6\n",
      "1   Telencephalon_L_501_5                  0.6\n",
      "2  CerebralCortex_L_482_4                  0.6\n",
      "3          Limbic_L_434_3                  0.6\n",
      "4           Hippo_L_338_2                  0.6\n",
      "6            Hippo_L_75_1                  0.6\n",
      "\u001b[1mTesting case 3:\u001b[0m\n",
      "                Structure  False negative rate\n",
      "0              Everything                  0.6\n",
      "1   Telencephalon_L_501_5                  0.6\n",
      "2  CerebralCortex_L_482_4                  0.6\n",
      "3          Limbic_L_434_3                  0.6\n",
      "4           Hippo_L_338_2                  0.6\n",
      "5            Amyg_L_336_2                  0.8\n",
      "6            Hippo_L_75_1                  0.6\n",
      "7             Amyg_L_73_1                  0.8\n",
      "\u001b[1mTesting case 4:\u001b[0m\n",
      "                Structure  False negative rate\n",
      "0              Everything                  0.6\n",
      "1   Telencephalon_L_501_5                  0.6\n",
      "2  CerebralCortex_L_482_4                  0.6\n",
      "3          Limbic_L_434_3                  0.6\n",
      "4           Hippo_L_338_2                  0.6\n",
      "5            Amyg_L_336_2                  0.8\n",
      "6            Hippo_L_75_1                  0.6\n",
      "7             Amyg_L_73_1                  0.8\n"
     ]
    }
   ],
   "source": [
    "# Viewing the false negative rate outputs\n",
    "\n",
    "print(\"\\033[1m\" + \"Testing case 2:\" + \"\\033[0m\")\n",
    "print(false_negative_rate(subset, file_names = [\"test1_results.npz\", \"test2_results.npz\", \"test3_results.npz\", \"test4_results.npz\"], case=2, n_repeats=10))\n",
    "\n",
    "print(\"\\033[1m\" + \"Testing case 3:\" + \"\\033[0m\")\n",
    "print(false_negative_rate(subset, file_names = [\"test1_results.npz\", \"test2_results.npz\", \"test3_results.npz\", \"test4_results.npz\"], case=3, n_repeats=10))\n",
    "\n",
    "print(\"\\033[1m\" + \"Testing case 4:\" + \"\\033[0m\")\n",
    "print(false_negative_rate(subset, file_names = [\"test1_results.npz\", \"test2_results.npz\", \"test3_results.npz\", \"test4_results.npz\"], case=4, n_repeats=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f2ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
